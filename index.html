<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="shortcut icon" href="https://zhishengzhong.com/myIcon.ico">


<meta name="keywords" content="Xin Lai, CSE, CUHK, Chinese University of Hong Kong">
<meta name="description" content="Xin Lai&#39;s home page">
<!-- <meta name="google-site-verification" content="X2QFrl-bPeg9AdlMt4VKT9v6MJUSTCf-SrY3CvKt4Zs"> -->
<link rel="stylesheet" href="./files/jemdoc.css" type="text/css">
<!-- <link rel="icon" type="image/png" href="https://zhishengzhong.com/pic/others/cuhk_logo.png"> -->
<title>Xin Lai's Homepage</title>
<!-- Google Analytics -->
<script async="" src="./files/analytics.js"></script><script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
<script type="text/javascript" src="./files/jquery.min.js"></script></head>
<body>
<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1>Xin Lai <font face="verdana"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; èµ–&nbsp;æ˜•</font></h1></div>
					<h3>Research Scientist</h3>
				<p>
					TikTok<br>
					ByteDance<br>
					Email: <a href="mailto:laixin1998@outlook.com">laixin1998@outlook[DOT]com</a>
				</p>
				<p>
					<a href="https://scholar.google.com/citations?user=tqNDPA4AAAAJ&hl=en&amp;hl=en"><img src="./files/google_scholar_logo.png" height="30px"></a>&nbsp;&nbsp;
				</p>
				<p></p>
			</td>
			<td>
				<img src="./files/xinlai.jpg" border="0" width="150"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<h2>Biography [<a href="./files/CV.pdf">CV</a>]</h2>
<!-- <h2>Biography </h2> -->
<p>
	</p><p>
    I am currently a researcher at TikTok, researching on Large Multimodal Models (LMMs). Before that, I obtained my Ph.D degree in 2024 from the Chinese University of Hong Kong (CUHK), supervised by <a href="https://jiaya.me/">Prof. Jiaya Jia</a> and <a href="https://lwwangcse.github.io/">Prof. Liwei Wang</a>.
	I also received the Bachelor's Degree at <a href="http://encs.hit.edu.cn/">Harbin Institute of Technology (HIT)</a> in 2020.
  	</p><p>
	My current research interest includes multimodal understanding and multimodal agents.
	</p><p>
	<b>We are actively <dev style="color:red;">hiring research interns</dev> in vision-language fundation models. If you're interested, welcome to contact me by email with your resume.</b>
	</p>
<p></p>


<h2>Recent News</h2>
<table id="tbTeaching" border="0" width="100%">
	<ul>
		<li>
			[2025/09] ðŸ”¥ We release <a href="https://github.com/Mini-o3/Mini-o3">Mini-o3</a> and a full training recipe to reproduce <dev style="color:red;">OpenAI o3-style thinking-with-images capability</dev>. Welcome to check it out! <a href="https://mini-o3.github.io/">[Project Page]</a>
		</li>
		<li>
			[2024/06] ðŸ”¥ We release <a href="https://github.com/dvlab-research/Step-DPO">Step-DPO</a>, a simple, effective, and data-efficient way to significantly enhance the mathematical reasoning abilities of LLMs. Welcome to the <a href="https://github.com/dvlab-research/Step-DPO">GitHub Page</a>! <dev style="color:red;">The resulting model achieves 70.8% and 94.0% accuracy on MATH and GSM8K, respectively, outperforming GPT-4-1106, Gemini-1.5-Pro, Claude-3-Opus!</dev>
		</li>
		<li>
			[2023/08] ðŸ”¥ We release <a href="https://github.com/dvlab-research/LISA">LISA</a> that unlocks the reasoning segmentation capabilities for multi-modal LLMs! Welcome to try <a href="http://103.170.5.190:7860/">Online Demo.</a> <dev style="color:red;">Over 1,500 GitHub Stars!</dev>
		</li>
		<li>
			[2023/07] <a href="https://github.com/dvlab-research/Mask-Attention-Free-Transformer">Mask-Attention-Free Transformer</a> is accepted by ICCV 2023.
		</li>
		<li>
			[2023/03] We release <a href="https://github.com/dvlab-research/SparseTransformer">Sparse Transformer</a>: fast, memory-efficient, and easy-to-use implementation for window-based 3D Transformer, well optimized by low-level CUDA code.
		</li>
		<li>
			[2023/03] <a href="https://github.com/dvlab-research/SphereFormer">Spherical Transformer</a> accepted by CVPR 2023: significantly enhance distant objects recognition for LiDAR points.
		</li>
		<li>
			[2022/03] ðŸ”¥ <a href="https://github.com/dvlab-research/Stratified-Transformer">Stratified Transformer</a> accepted by CVPR 2022: <dev style="color:red;">a pioneering fully transformer-based 3D fundamental network.</dev>
		</li>
	</ul>
	<!-- <tbody>
		<tr>
			<td>Aug., 2023</td> <td><a href="https://github.com/dvlab-research/LISA">LISA</a>: Unlocking the reasoning segmentation capabilities for multi-modal LLMs! Welcome to try <a href="http://103.170.5.190:7860/">Online Demo</a>.</td>
		</tr>
		<tr>
			<td>Jul., 2023</td> <td>Mask-Attention-Free Transformer is accepted by ICCV 2023.</td>
		</tr>
		<tr>
			<td>Mar., 2023</td> <td><a href="https://github.com/dvlab-research/SphereFormer">Spherical Transformer</a> accepted by CVPR 2023: significantly enhance distant objects recognition for LiDAR points.</td>
		</tr>
		<tr>
			<td>Mar., 2022</td> <td><a href="https://github.com/dvlab-research/Stratified-Transformer">Stratified Transformer</a> accepted by CVPR 2022: a pioneering fully transformer-based 3D fundamental network.</td>
		</tr>
	</tbody> -->
</table>





<h2> Selected Publications [<a href="https://scholar.google.com/citations?user=tqNDPA4AAAAJ&hl=en&amp;hl=en">Google Scholar</a>]</h2>
<table id="tbPublications" width="100%" style="border-collapse:separate; border-spacing:0px 10px;">
	<tbody>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/mini-o3.png"></td>
		<td>
			<div><b>Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search</b></div>
			<div style="font-size: 15px"><b>Xin Lai</b>*, Junyi Li*, Wei Li, Tao Liu, Tianjian Li, Hengshuang Zhao</div>
			<div>arXiv pre-print</div>
			<div>[<a href="https://arxiv.org/pdf/2509.07969"><b>paper</b></a>|<a href="https://github.com/Mini-o3/Mini-o3"><b>code</b></a>|<a href="https://mini-o3.github.io/"><b>project page</b></a>]</div>
			<div><p style="color:red;">A full training recipe to reproduce OpenAI o3-style thinking-with-images capability.</p></div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/lisa.png"></td>
		<td>
			<div><b>LISA: Reasoning Segmentation via Large Language Model</b></div>
			<div style="font-size: 15px"><b>Xin Lai*</b>, Zhuotao Tian*, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia</div>
			<div>CVPR, 2024 <b style="color:red;">(Oral, 3.3% acceptance rate)</b></div>
			<div>[<a href="https://arxiv.org/pdf/2308.00692.pdf"><b>paper</b></a>|<a href="https://github.com/dvlab-research/LISA.git"><b>code</b></a>]</div>
			<div><p style="color:red;">Over 2,400 GitHub Stars!</p></div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/vision_think.png"></td>
		<td>
			<div><b>VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning</b></div>
			<div style="font-size: 15px">Senqiao Yang*, Junyi Li*, <b>Xin Lai</b>*, Bei Yu, Hengshuang Zhao, Jiaya Jia</div>
			<div>arXiv pre-print</div>
			<div>[<a href="https://arxiv.org/abs/2507.13348"><b>paper</b></a>|<a href="https://github.com/dvlab-research/VisionThink"><b>code</b></a>]</div>
			<div>A new paradigm of efficient VLMs with token compression.</p></div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/step_dpo.png"></td>
		<td>
			<div><b>Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs</b></div>
			<div style="font-size: 15px"><b>Xin Lai</b>, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, Jiaya Jia</div>
			<div>arXiv pre-print</div>
			<div>[<a href="https://arxiv.org/pdf/2406.18629"><b>paper</b></a>|<a href="https://github.com/dvlab-research/Step-DPO"><b>code</b></a>]</div>
			<div><p style="color:red;">70.8% and 94.0% accuracy on MATH and GSM8K, respectively! Outperforms GPT-4-1106, Gemini-1.5-Pro, Claude-3-Opus!</p></div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/longlora.png"></td>
		<td>
			<div><b>LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</b></div>
			<div style="font-size: 15px">Yukang Chen, Shengju Qian, Haotian Tang, <b>Xin Lai</b>, Zhijian Liu, Song Han, Jiaya Jia</div>
			<div>ICLR, 2024 <b style="color:red;">(Oral Presentation)</b></div>
			<div>[<a href="https://arxiv.org/pdf/2309.12307"><b>paper</b></a>|<a href="https://github.com/dvlab-research/LongLoRA"><b>code</b></a>]</div>
			<div><p style="color:red;">Over 2,700 GitHub Stars!</p></div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/maft.png"></td>
		<td>
			<div><b>Mask-Attention-Free Transformer for 3D Instance Segmentation</b></div>
			<div style="font-size: 15px"><b>Xin Lai</b>, Yuhui Yuan, Ruihang Chu, Yukang Chen, Han Hu, Jiaya Jia</div>
            <div>ICCV, 2023</div>
			<div>[<a href="https://arxiv.org/pdf/2309.01692.pdf"><b>paper</b></a>|<a href="https://github.com/dvlab-research/Mask-Attention-Free-Transformer"><b>code</b></a>]</div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/sphereformer.png"></td>
		<td>
			<div><b>Spherical Transformer for LiDAR-based 3D Recognition</b></div>
			<div style="font-size: 15px"><b>Xin Lai</b>, Yukang Chen, Fanbin Lu, Jianhui Liu, Jiaya Jia</div>
            <div>CVPR, 2023</div>
			<div>[<a href="https://arxiv.org/pdf/2303.12766.pdf"><b>paper</b></a>|<a href="https://github.com/dvlab-research/SphereFormer"><b>code</b></a>]</div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/stratified.png"></td>
		<td>
			<div><b>Stratified Transformer for 3D Point Cloud Segmentation</b></div>
			<div style="font-size: 15px"><b>Xin Lai*</b>, Jianhui Liu*, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, Jiaya Jia</div>
            <div>CVPR, 2022</div>
			<div>[<a href="https://arxiv.org/pdf/2203.14508.pdf"><b>paper</b></a>|<a href="https://github.com/dvlab-research/Stratified-Transformer"><b>code</b></a>]</div>
			<div><p style="color:red;">A pioneering fully transformer-based 3D fundamental network.</p></div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/decouplenet.png"></td>
		<td>
			<div><b>DecoupleNet: Decoupled Network for Domain Adaptive Semantic Segmentation</b></div>
			<div style="font-size: 15px"><b>Xin Lai</b>, Zhuotao Tian, Xiaogang Xu, Yingcong Chen, Shu Liu, Hengshuang Zhao, Liwei Wang, Jiaya Jia</div>
            <div>ECCV, 2022</div>
			<div>[<a href="https://arxiv.org/pdf/2207.09988.pdf"><b>paper</b></a>|<a href="https://github.com/dvlab-research/DecoupleNet"><b>code</b></a>]</div>
		</td>
	</tr>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/cac.png"></td>
		<td style="font-size: 16px">
			<div><b>Semi-supervised Semantic Segmentation with Directional Context-aware Consistency</b></div>
			<div><b>Xin Lai</b>, Zhuotao Tian, Li Jiang, Shu Liu, Hengshuang Zhao, Liwei Wang, Jiaya Jia</div>
            <div>CVPR, 2021</div>
			[<a href="https://arxiv.org/pdf/2106.14133.pdf"><b>paper</b></a>|<a href="https://github.com/dvlab-research/Context-Aware-Consistency"><b>code</b></a>]
		</td>
	</tr>
    </tbody>
</table>


<h2> Open-source Projects </h2>
<table id="tbOpen-source" width="100%" style="border-collapse:separate; border-spacing:0px 10px;">
	<tbody>

	<tr>
		<td><img width="300" style="padding: 20px;" src="./files/sparse_transformer.png"></td>
		<td>
			<div><b>Sparse Transformer</b></div>
			<div style="font-size: 15px"><b>Xin Lai</b>, Fanbin Lu, Yukang Chen</div>
            <div>Open-source Library</div>
			<div>[<a href="https://github.com/dvlab-research/SparseTransformer"><b>code</b></a>]</div>
			<div><p style="color:red;">Fast, memory-efficient, and easy-to-use implementation for window-based 3D Transformer, well optimized by low-level CUDA code.</p></div>
		</td>
	</tr>

    </tbody>
</table>

<h2> Experiences</h2>
<ul>
    <table width="100%" align="center" border="0" cellpadding="10">
        <tbody><tr>
          <td width="25%" align="center">
            <img src="./files/Tencent.png" alt="face" width="80%">
          </td>
          <td width="75%" valign="center">
                <strong>July 2023 - Jan. 2024</strong>, <b> Tencent Co.,Ltd.</b><br> Internship, Mentor: <a href="https://yanpei.me/">Dr. Yanpei Cao</a> <br>
          </td>
        </tr>
        </tbody>
    </table>

	<table width="100%" align="center" border="0" cellpadding="10">
        <tbody><tr>
          <td width="25%" align="center">
            <img src="./files/MSRA.png" alt="face" width="80%">
          </td>
          <td width="75%" valign="center">
                <strong>Dec. 2022 - May 2023</strong>, <b>Microsoft Research Asia (MSRA)</b><br> Collaboration, Mentor: <a href="https://scholar.google.com/citations?user=PzyvzksAAAAJ&hl=en">Dr. Yuhui Yuan</a> <br>
          </td>
        </tr>
        </tbody>
    </table>
</ul>


<h2> Honors and Awards</h2>
<ul>
	<li>
		Outstanding Reviewer, CVPR, 2023
    </li>
	<li>
		Postgraduate Scholarship, CUHK, 2020-2024
    </li>
	<li>
		Outstanding Graduate, HIT, 2020
	</li>
    <li>
        National Scholarship (Top 1%), China, 2018
    </li>
    <li>
        Fung Scholarship, Hong Kong, 2018
    </li>
    <li>
        Provincial Merit Student (Top 2%), China, 2018
    </li>
    <li>
        People's Scholarship, HIT, 2016-2018
    </li>
</ul>

<h2>Professional Services</h2>

<li>	
	<b>Conference Services:</b><br>
	International Conference on Learning Representations (ICLRâ€™24)<br>
	Winter Conference on Applications of Computer Vision (WACVâ€™24)<br>
	Conference and Workshop on Neural Information Processing Systems (NeurIPSâ€™23)<br>
	IEEE Conference on Computer Vision and Pattern Recognition (CVPRâ€™22,23)<br>
	IEEE International Conference on Computer Vision (ICCVâ€™21,23)<br>
	European Conference on Computer Vision (ECCVâ€™22)<br>
	<p style="margin-top:3px"></p>
</li>

<li>
    <b>Journal Reviews:</b><br>
	IEEE Transactions on Image Processing (TIP)<br>
	Pattern Recognition (PR)<br>
	<p style="margin-top:3px"></p>		
</li>


<h2>Teaching</h2>
<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td> 2021-2022</td><td>Fall</td><td>ENGG 1110: Problem Solving By Programming</td>
		</tr>
		<tr>
			<td> 2020-2021</td><td>Spring</td><td>CSCI 3251: Engineering Practicum</td>
		</tr>
		<tr>
			<td> 2020-2021</td><td>Fall</td><td>ENGG 1110: Problem Solving By Programming</td>
		</tr>
	</tbody>
</table>

<div id="footer">
	<div id="footer-text"></div>
</div>    
        Â© Xin Lai | Last updated: March 2024
</div>

</body></html>