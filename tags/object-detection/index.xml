<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>object detection | Xin LAI</title>
    <link>https://x-lai.github.io/tags/object-detection/</link>
      <atom:link href="https://x-lai.github.io/tags/object-detection/index.xml" rel="self" type="application/rss+xml" />
    <description>object detection</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 06 Nov 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://x-lai.github.io/img/icon-192.png</url>
      <title>object detection</title>
      <link>https://x-lai.github.io/tags/object-detection/</link>
    </image>
    
    <item>
      <title>RefineDet</title>
      <link>https://x-lai.github.io/post/refinedet/</link>
      <pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://x-lai.github.io/post/refinedet/</guid>
      <description>

&lt;h1 id=&#34;single-shot-refinement-neural-network-for-object-detection&#34;&gt;Single-Shot Refinement Neural Network for Object Detection&lt;/h1&gt;

&lt;p&gt;RefineDet综合了two-stage和one-stage的优点，同时规避其缺点，是一种既准又快的目标检测器。two-stage的检测器有3个优点：1）two-stage structure with sampling heuristics帮助解决class imbalance问题; 2) 有两次对bbox的回归；3）有两阶段对特征的表示（把两个阶段看作两个任务，可以把整个学习看作多任务学习，所以第一个阶段的学习会加强浅层特征）。RefineDet就借助了这几点，能够在第一阶段reject掉easy negative examples，并且先进行一个粗略的bbox regress和pos/neg的打分，再从第二阶段进行精细的regression和classification。&lt;/p&gt;

&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;类似于FPN，bottom-up部分称为ARM（anchor refinement module），top-down部分称作ODM（object detection module）。两个module都将各个level的feature maps用于预测和分类。ARM进行bbox的粗略预测和binary classification，从而得到coarse bbox坐标和pos/neg打分。然后挑出negative confidence大于某个threshold的那些easy negative examples，再把剩下的anchor送到第二阶段ODM处理。&lt;/p&gt;

&lt;h2 id=&#34;和faster-rcnn对比-以及一些思考&#34;&gt;和Faster RCNN对比，以及一些思考&lt;/h2&gt;

&lt;p&gt;1.RefineDet的第一阶段ARM是在FPN的bottom-up部分做的，而Faster RCNN的第一阶段是在top-down部分做的，所以将ARM放在top-down做是否会更好？&lt;/p&gt;

&lt;p&gt;2.RefineDet将easy negative examples放到后面ODM处理时是不计算它们的损失，从而缓解class imbalance的影响。而Faster RCNN的第二阶段是用roi把每个proposal都用RoI pooling割成固定长度的向量，所以效率低。而前者仍然基于one-stage的框架来做。&lt;/p&gt;

&lt;p&gt;3.把整个regression分两步做，是不是会让第一阶段ARM的训练使得浅层的特征加强，从而导致整体效果的增长？如果是这样的话，是不是其他任务也可以分步来做（用来加强特征），或者再多加一步变成分3步？或者在更浅的层（backbone的部分）再加一步？&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DenseBox</title>
      <link>https://x-lai.github.io/post/densebox/</link>
      <pubDate>Mon, 28 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://x-lai.github.io/post/densebox/</guid>
      <description>

&lt;h1 id=&#34;densebox-unifying-landmark-localization-with-end-to-end-object-detection&#34;&gt;DenseBox: Unifying Landmark Localization with End to End Object Detection&lt;/h1&gt;

&lt;p&gt;DenseBox直接通过FCN框架来同时预测定位以及分类。它主要有2个方面值得学习：1）DenseBox不需要手动设置anchor，根据receptive field来设置ground-truth，通过一种类似于segmentation的方法来训练；2）通过multi-task learning加入landmarks信息，提高检测精度。&lt;/p&gt;

&lt;h2 id=&#34;框架&#34;&gt;框架&lt;/h2&gt;

&lt;p&gt;输入$m\times n$的image, 输出5个$\frac{m}{4} \times \frac{n}{4}$的feature maps。其中一个用来输出object confidence，其他四个用来标记bounding box。对于output feature map的每个pixel$(x_i, y_i)$，通过一个五元组来描述bounding box$(\hat{s},\hat{dx^t}=\hat{x_i}-x_t,\hat{dy^t}=\hat{y_i}-y_t,\hat{dx^b}=\hat{x_i}-x_b, \hat{dy^b}=\hat{y_i}-y_b)$。其中$\hat{s}$是object confidence，$(x_t,y_t)$是bounding box的左上角坐标，$(x_b,y_b)$是bounding box的右下角坐标。&lt;/p&gt;

&lt;p&gt;如何标记ground-truth：在output feature map上，对于一个人脸的bounding box，取其长度的0.3作为半径，并以其中心为圆心，作一个圆，该圆的区域为positive，其他区域为negative。&lt;/p&gt;

&lt;p&gt;为什么这样设计ground-truth？因为在该圆的区域内的每个pixel的receptive field都大致是以这个object（即人脸）为中心的。&lt;/p&gt;

&lt;h2 id=&#34;multi-task-training&#34;&gt;Multi-task training&lt;/h2&gt;

&lt;p&gt;没使用landmarks的情况下，其loss function为：&lt;/p&gt;

&lt;p&gt;$L_{det}(\theta)=\sum_i({M(\hat{t_i})L_{cls}(\hat{t_i},y_i^*)}+\lambda_{loc}[y_i^*&amp;gt;0]L_{loc}(\hat{d_i}, d_i^*))$&lt;/p&gt;

&lt;p&gt;使用landmarks之后，再添加一个分支（该分支也用上了用于detection的特征图），该分支输出N个feature  maps（其中N表示每个object有N个landmarks）。loss function变为：&lt;/p&gt;

&lt;p&gt;$L_{full}(\theta)=\lambda_{det}L_{det}(\theta)+\lambda_{lm}L_{lm}(\theta)+L_{rf}(\theta)$&lt;/p&gt;

&lt;p&gt;其中$L_{lm}(\theta)$表示landmark regression loss（用于回归的，与$L_{loc}$类似），$L_{rf}(\theta)$表示refined detection loss（用于分类的，与$L_{cls}$类似）。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MRCNN</title>
      <link>https://x-lai.github.io/post/mrcnn/</link>
      <pubDate>Sun, 27 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://x-lai.github.io/post/mrcnn/</guid>
      <description>

&lt;h1 id=&#34;object-detection-via-a-multi-region-semantic-segmentation-aware-cnn-model&#34;&gt;Object detection via a multi-region &amp;amp; semantic segmentation-aware CNN model&lt;/h1&gt;

&lt;p&gt;本文提出一种新的CNN模型，包括了multi-region CNN model以及semantic segmentation-aware CNN model。同时还提出一种iterative localization通过不断迭代得到精确的localization regression。&lt;/p&gt;

&lt;h2 id=&#34;multi-region-cnn-model&#34;&gt;Multi-Region CNN Model&lt;/h2&gt;

&lt;p&gt;整体框架包括两部分:&lt;/p&gt;

&lt;p&gt;1) Activation maps module: 用于从整幅输入图像中提取整体的特征图；&lt;/p&gt;

&lt;p&gt;2) Region adaptation module: 用于从整体特征图中截取proposal region各个部分的特征，并使用spatially adaptive (max-) pooling，然后分别放入对应的网络中。&lt;/p&gt;

&lt;p&gt;最终，candidate box representation通过将所有region adaptation module的最后一层concatenate起来得到。&lt;/p&gt;

&lt;p&gt;这样做，是为了：1）强制得到各个每个RoI各个部分的表示，从而得到一个much richer and more robust object representation；2）使得最终的object representation more sensitive to inaccurate localization。&lt;/p&gt;

&lt;h3 id=&#34;role-in-detection&#34;&gt;Role in detection&lt;/h3&gt;

&lt;p&gt;1) Discriminative feature diversification. 使用多个区域而不是整个RoI的好处在于可以使得网络集中于某一块小区域，而非一整块区域。经过实验证明，遮住一部分而将注意力集中在一小块能使得检测准确率上升；&lt;/p&gt;

&lt;p&gt;2) Localization-aware representation. 使用多区域的结构可以缓解定位不准的问题，因为使用多区域会使得检测器专注于某几小块，从而对定位更加敏感（即如果定位不准确的话，会对分类产生较大偏差）。此外，使用多区域实际上impose soft constraints regarding the visual content allowed in each type of region，这种soft constraints对定位有一定帮助。&lt;/p&gt;

&lt;h2 id=&#34;semantic-segmentation-aware-cnn-model&#34;&gt;Semantic Segmentation-Aware CNN Model&lt;/h2&gt;

&lt;p&gt;由于segmentation任务和detection任务相似，本文提出使用semantic segmentation任务来提取特征，用来辅助检测，而且，这种方法不需要额外的分割所需的ground truth annotated data。&lt;/p&gt;

&lt;p&gt;使用FCN训练分割模型。直接将object detection的每个object对应的bounding box内的像素设置为foreground class。其余设为background class。以此获得训练数据。训练完成后，将最后一层分类层去掉，并用剩下的来提取semantic segmentation-aware features.&lt;/p&gt;

&lt;h2 id=&#34;object-localization&#34;&gt;Object Localization&lt;/h2&gt;

&lt;h3 id=&#34;cnn-region-adaptation-module-for-bounding-box-regression&#34;&gt;CNN region adaptation module for bounding box regression&lt;/h3&gt;

&lt;p&gt;在上述模型中再额外添加一个module用来预测bounding box。对于每一类，都输出4个数。并且为了能够refine那些与真实相差较大的box，本文提出先将原region扩大1.3倍，然后再预测。&lt;/p&gt;

&lt;h3 id=&#34;iterative-localization-bounding-box-voting&#34;&gt;Iterative Localization &amp;amp; Bounding Box Voting&lt;/h3&gt;

&lt;p&gt;详见原文&lt;/p&gt;

&lt;h2 id=&#34;一些想法&#34;&gt;一些想法&lt;/h2&gt;

&lt;p&gt;能否通过mask来学到不同的部位，并结合其特征，并实现end-to-end的训练？&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R-FCN</title>
      <link>https://x-lai.github.io/post/r-fcn/</link>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://x-lai.github.io/post/r-fcn/</guid>
      <description>

&lt;h1 id=&#34;r-fcn-object-detection-via-region-based-fully-convolutional-networks&#34;&gt;R-FCN: Object Detection via Region-based Fully Convolutional Networks&lt;/h1&gt;

&lt;p&gt;Faster R-CNN之所以比较慢，是因为RPN输出的每个proposal都需要经过detection network。本文提出position-sensitive score maps使得RPN输出的所有的proposal共享计算，从而提高效率。&lt;/p&gt;

&lt;p&gt;Fast R-CNN或Faster R-CNN能够具有很强的translation variance的能力，主要在于RoI pooling对信息的整合，并形成一个fixed-size的而且携带位置信息(因为RoI pooling layer将feature map spatially分成若干块)的向量。但这样是以unshared per-RoI computation为代价的。（原文说得更加透彻：This region-specific operation(指RoI pooling) breaks down translation invariance, and the post-RoI convolutional layers are no longer translation-invariant when evaluated across different regions.）&lt;/p&gt;

&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;R-FCN的所有learnable weight layers都是convolutional的，并且是在entire image中计算的。最后一个卷积层对每一类都产生$k^2$个position-sensitive score maps，所以输出层有$k^2(C+1)$个channel（+1是为了预测出background）。$k^2$是对应了每个RoI都被切割成$k^2$块分别给对应的位置。&lt;/p&gt;

&lt;p&gt;R-FCN以position-sensitive RoI pooling layer结束。对每个RoI，position-sensitive RoI pooling layer将最后一个卷积层的输出聚集起来，并且打分。每个RoI经过这个pooling layer之后都会得到$C+1$个channel，并且每个channel都是$k\times k$的。每个channel都对应着一个类。而且对每个channel，这$k\times k$个bin中的每一个都只来自于R-FCN输出的$k^2$个position-sensitive score maps中的一个。&lt;/p&gt;

&lt;h2 id=&#34;position-sensitive-score-maps-position-sensitive-roi-pooling&#34;&gt;Position-sensitive score maps &amp;amp; Position-sensitive RoI pooling&lt;/h2&gt;

&lt;p&gt;将每个RoI矩形划分成$k \times k$个bins。对于一个形如$w\times h$的矩形，每个bin的大小约等于$\frac{w}{k}\times \frac{h}{k}$。在第$(i,j)$个bin$(0\le i,j \le k-1)$中，定义position-sensitive RoI pooling如下：&lt;/p&gt;

&lt;p&gt;$r_c(i,j|\Theta)=\sum_{(x,y)\in bin(i,j)}{z_{i,j,c}(x+x_0,y+y_0|\Theta)}/n$&lt;/p&gt;

&lt;p&gt;其中$r_c(i,j)$表示pooling的结果中对第c类的第$(i,j)$个bin的值，$z_{i,j,c}$表示那$k^2(C+1)$个score maps中第c类第$(i,j)$个bin对应的那个，$(x_0,y_0)$表示RoI的左上角在score map上的坐标。本文采用的是average pooling，所以除以n，当然也可以用max pooling。&lt;/p&gt;

&lt;p&gt;最后，对pooling得到的C+1个channel分别做average voting。即$r_c(\Theta)=\sum_{i,j}{r_c(i,j|\Theta)}$。&lt;/p&gt;

&lt;p&gt;同样的，除了上述$k^2(C+1)$维的卷积层，还有一个$4k^2$的卷积层和它平行，用于预测坐标。positive-sensitive RoI pooling作用在这$4k^2$个channel上，得到4个大小为$k^2$的channel，最终通过average voting得到4维向量即$(t_x, t_y, t_w, t_h)$。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FPN</title>
      <link>https://x-lai.github.io/post/fpn/</link>
      <pubDate>Sat, 12 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://x-lai.github.io/post/fpn/</guid>
      <description>

&lt;h1 id=&#34;feature-pyramid-networks-for-object-detection&#34;&gt;Feature Pyramid Networks for Object Detection&lt;/h1&gt;

&lt;p&gt;feature pyramids对于识别不同scale的物体有着重要的作用。但是当feature map随着网络的深度由于subsampling(pooling)的操作而变得越来越小时，虽然构建了feature pyramids，但是深度较浅的feature map尽管resolution更大，但low-level信息比较多而且semantics较少（low-level信息多是不利的，semantics少也是不利的），而深度较深的feature map尽管low-level信息较少semantics较多，但resolution很小（这不适于预测小物体），所以造成这种feature pyramids效果的瓶颈。本文提出通过一个带有lateral connections的top-down网络结构，用于构建既有fine resolution又富含semantics的feature maps（即每个level的feature map都是富含semantics的）。&lt;/p&gt;

&lt;h2 id=&#34;feature-pyramid-networks&#34;&gt;Feature Pyramid Networks&lt;/h2&gt;

&lt;p&gt;整个网络结构包含两部分:1) Bottom-up pathway, 2) Top-down pathway and lateral connections&lt;/p&gt;

&lt;h3 id=&#34;bottom-up-pathway&#34;&gt;Bottom-up pathway&lt;/h3&gt;

&lt;p&gt;这是feature pyramids的自底至上的subsampling的过程。每个scale不止一层，而是多个卷积层。并且将一个scale对应的多个层（称为stage）的最后一层的输出作为该scale的代表。每个scale的scaling step是2，也即相邻scale的feature map的size之比为2。&lt;/p&gt;

&lt;h3 id=&#34;top-down-pathway-and-lateral-connections&#34;&gt;Top-down pathway and lateral connections&lt;/h3&gt;

&lt;p&gt;Top-down pathway是将coarse-resolution的feature map逐级地upsampling得到higher resolution的过程。这种upsampling在本文中直接使用nearest neighbor upsampling。在upsampling（upsampling factor=2）的同时，还会结合（element-wise addition操作）一个来自于bottom-up pathway的pyramids中相同size的feature map的lateral connection（需要将这个来自bottom-up的feature map进行$1 \times 1$的卷积以便改变channel dimensions）。upsampling的结果虽然resolution更加fine了，semantics也很多，但是由于upsampling的随机性，又是less accurately localized。而来自于bottom-up的feature map却恰恰相反，虽然更加accurately localized，但是semantics更少。所以将他们结合起来，会得到既accurately localized又semantically strong的feature map。&lt;/p&gt;

&lt;h2 id=&#34;feature-pyramid-networks-for-rpn&#34;&gt;Feature Pyramid Networks for RPN&lt;/h2&gt;

&lt;p&gt;对于使用FPN的RPN，本文设计将所有anchor分散到不同scale的feature map上，以便feature pyramids上的每个scale都有自己的分工。并且对不同的scale的feature map，RPN的head参数都是共享的。实验表明，当不共享head参数时，准确度是相似的。这也说明FPN产生的不同level的feature maps有similar semantic levels。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RetinaNet</title>
      <link>https://x-lai.github.io/post/retinanet/</link>
      <pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://x-lai.github.io/post/retinanet/</guid>
      <description>

&lt;h1 id=&#34;focal-loss-for-dense-object-detection&#34;&gt;Focal Loss for Dense Object Detection&lt;/h1&gt;

&lt;p&gt;one-stage的检测器比two-stage的更快更简单，但是在准确度上落后。本文发现最主要的原因在于极度的foreground与background的比例失衡（class imbalance）。由此，本文对标准的cross entropy进行修改，并提出Focal Loss。这种loss function可以减小easy examples占的整体loss的比重。这样就使得模型在hard examples下训练，从而减小大量easy examples对训练的影响，解决class imbalance的问题。此外，本文还提出RetinaNet。&lt;/p&gt;

&lt;h2 id=&#34;r-cnn这种two-stage的检测器如何解决class-imbalance&#34;&gt;R-CNN这种two-stage的检测器如何解决class imbalance？&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;a two-stage cascade。像R-CNN这种two-stage的模型先region proposal，然后再对proposal的结果进行分类。这中间形成了一种级联，使得在第一步region proposal的过程（如RPN，selective search）中就筛除了大量的easy examples（这种过程也可视为一种sampling heuristics）。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在第二阶段分类的时候，也使用了sampling heuristics（如fixed foreground-background ratio 1:3 和online hard example mining）。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而one-stage detector却需要处理更多的candidate object locations，通常会有～100k个candidate locations。尽管one-stage的检测器也可以使用上述第2点的sampling heuristics，但是由于基数过大，还是会造成训练过程被easy examples主导，由此导致低效以及模型的泛化能力降低。&lt;/p&gt;

&lt;h2 id=&#34;focal-loss&#34;&gt;Focal Loss&lt;/h2&gt;

&lt;p&gt;对于二分类而言，cross entropy表达式如下：&lt;/p&gt;

&lt;p&gt;$$CE(p,y)=\left\{ \begin{aligned} &amp;amp; -log(p) &amp;amp; &amp;amp; if \  y=1 \\ &amp;amp; -log(1-p) &amp;amp; &amp;amp; otherwise \end{aligned}\right.$$&lt;/p&gt;

&lt;p&gt;其中p表示预测到y=1的那一类的概率。为了方便，定义$p_t$:&lt;/p&gt;

&lt;p&gt;$$p_t=\left\{\begin{aligned} &amp;amp; p &amp;amp; &amp;amp; if \ y=1 \\
&amp;amp; 1-p &amp;amp; &amp;amp; otherwise \end{aligned}\right.$$&lt;/p&gt;

&lt;h3 id=&#34;balanced-cross-entropy&#34;&gt;Balanced Cross Entropy&lt;/h3&gt;

&lt;p&gt;Balanced Cross Entropy对标准cross entropy进行了改造：&lt;/p&gt;

&lt;p&gt;$CE(p_t)=-\alpha_tlog(p_t)$&lt;/p&gt;

&lt;p&gt;其中当y=1时，$\alpha_t=\alpha$，而当y=-1时，$\alpha_t=1-\alpha$。&lt;/p&gt;

&lt;p&gt;这样做之后，可以通过调节$\alpha$的值来调节positive-negtive samples分别对loss的贡献的比例。但是这样并不能区分hard examples和easy examples对loss的贡献。所以引入了Focal Loss。&lt;/p&gt;

&lt;h3 id=&#34;focal-loss-1&#34;&gt;Focal Loss&lt;/h3&gt;

&lt;p&gt;$FL(p_t)=-(1-p_t)^\gamma log(p_t)$&lt;/p&gt;

&lt;p&gt;Focal Loss在cross entropy的基础上添加了一个权重$(1-p_t)^\gamma$。这个权重可用于调整不同样本对loss的贡献。对于y=-1而言，一个easy example的预测结果将会使$p_t$增大，从而使得其权重$(1-p_t)^\gamma$减小，所以easy example的权重会减小。&lt;/p&gt;

&lt;p&gt;在实践中，Focal loss通常会与balanced cross entropy结合起来:&lt;/p&gt;

&lt;p&gt;$FL(p_t)=-\alpha_t(1-p_t)^\gamma log(p_t)$&lt;/p&gt;

&lt;h3 id=&#34;class-imbalance-and-model-initialization&#34;&gt;Class Imbalance and Model Initialization&lt;/h3&gt;

&lt;p&gt;二分类通常会默认将模型初始化使得起初分类到y=1和y=-1的概率相等。但是在class imbalance的情况下，将两类的初始预测值设为相同会使得frequent class的loss主导整个loss，使得训练不稳定。于是，本文设计将rare class初始预测概率设定为一个prior $\pi$，在本文中$\pi=0.01$。这样设定之后，在class imbalance的情况下，正负两类所占的loss相差就不会太大。这样的initialization会使得训练过程更加smooth。&lt;/p&gt;

&lt;h2 id=&#34;retinanet&#34;&gt;RetinaNet&lt;/h2&gt;

&lt;p&gt;RetinaNet采用FPN的结构，使用ResNet作为backbone。并且延续了RPN的anchor设计。值得一提的是，对于每个level的feature maps，所有的classification subnet和box subnet都是共享参数的。并且class subnet和box subnet都是使用了$3\times3$的卷积核。但是class subnet和box subnet并不共享参数。另外，class subnet的最后一层是sigmoid，而不是softmax。这样处理的目的是为了兼容一个anchor对应多个标签。然后损失函数相当于对每个类别都进行二分类。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SSD</title>
      <link>https://x-lai.github.io/post/ssd/</link>
      <pubDate>Thu, 12 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://x-lai.github.io/post/ssd/</guid>
      <description>

&lt;h1 id=&#34;ssd-single-shot-multibox-detector&#34;&gt;SSD: Single Shot MultiBox Detector&lt;/h1&gt;

&lt;p&gt;和YOLO一样，SSD也是一种one-stage的object detector。SSD将prediction bounding boxes的输出空间离散化到多个feature maps上每个位置的多个不同scale和aspect ratios的default boxes上。&lt;/p&gt;

&lt;h2 id=&#34;framework&#34;&gt;Framework&lt;/h2&gt;

&lt;p&gt;SSD在训练时只需要输入图片和对每个object的gt boxes。SSD将会在不同resolution的feature maps上来预测每个location使用哪个scale和aspect ratio的default boxes。对于每个default box，SSD将会预测shape offsets以及object category confidences（包括background类）。在训练的时候，先将这些default boxes match到gt boxes上，确定哪些default boxes是positive（即被认为是包含object的），剩余的default boxes就被作为negatives。然后再结合prediction来计算loss，进行训练。&lt;/p&gt;

&lt;h2 id=&#34;如何match-default-boxes&#34;&gt;如何match default boxes&lt;/h2&gt;

&lt;p&gt;与YOLO v2不同的是，SSD提出不一定要对每个gt box只match一个default box(anchor box)。SSD先将每个gt box match到与它自己IOU最大的那个default box上，然后再将那些与任一gt box的IOU超过某个threshold（本文设为0.5）的default boxes也标记对应到该gt box/objectness/class。&lt;/p&gt;

&lt;h2 id=&#34;multi-scale-feature-maps-for-detection&#34;&gt;Multi-scale feature maps for detection&lt;/h2&gt;

&lt;p&gt;在预测的时候，SSD使用多种不同scale的feature maps来进行detection。&lt;/p&gt;

&lt;h2 id=&#34;convolutional-predictor-for-detection&#34;&gt;Convolutional predictor for detection&lt;/h2&gt;

&lt;p&gt;对于所有不同scale的feature map，都将采用若干个3*3的filter来进行卷积，从而进行预测。&lt;/p&gt;

&lt;h2 id=&#34;default-boxes-and-aspect-ratios&#34;&gt;Default boxes and aspect ratios&lt;/h2&gt;

&lt;p&gt;对于feature map的每个cell的每个default box，SSD都会预测offsets以及classification。这样每种feature map就会有$(c+4)k$个filters（假设一共有c-1个class，另外加上background共有c个class）。其中k为default boxes的个数（本文取6）。&lt;/p&gt;

&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;

&lt;h3 id=&#34;training-objective&#34;&gt;Training Objective&lt;/h3&gt;

&lt;p&gt;根据之前‘如何match default boxes’所述，我们可以获得哪些default boxes是positives或negatives。随后即可计算loss function。$L(x,c,l,g)=\frac{1}{N} (L_{conf}(x,c)+\alpha{L_{loc}(x,l,g)})$。其中$x_{ij}^k$要么取1，要么取0，表示第i个default box是否和第j个gt box(其category为k) match，$c,l,g$分别表示预测的class，预测的box以及预测的gt box。$L_{loc}$是使用和faster R-CNN类似的localization error。而$L_{conf}(x,c)=-\sum_{i\in Positives}^{N}x_{ij}^plog(\hat{c_i^p})-\sum_{i\in Negatives}{log(\hat{c_i^0})}, \hat{c_i^p}=\frac{exp(c_i^p)}{\sum_p{exp(c_i^p)}}$。&lt;/p&gt;

&lt;h3 id=&#34;choose-scales-and-aspect-ratios-for-default-boxes&#34;&gt;Choose scales and aspect ratios for default boxes&lt;/h3&gt;

&lt;p&gt;设共有m个不同scale的feature maps，那么每个feature map的default boxes的scale就用如下公式计算：$s_k=s_{min}+\frac{s_{max}-s_{min}}{m-1}(k-1),\quad k\in[1,m]$。本文取$s_{min}=0.2, s_{max}=0.9$。另外再取aspect ratios。$a_r\in \{1,2,3,\frac{1}{2},\frac{1}{3}\}$，第k个feature map的default box的宽$w_k^a$和高$h_k^a$分别用$w_k^a=s_k\sqrt{a_r},h_k^a=s_k\sqrt{a_r}$来计算。另外对于aspect ratio=1的情况，还要再加一种size：$s_{k}&amp;lsquo;=\sqrt{s_ks_{k+1}}$。&lt;/p&gt;

&lt;h3 id=&#34;hard-negative-mining&#34;&gt;Hard negative mining&lt;/h3&gt;

&lt;p&gt;对所有negative default boxes对应的confidence按高到低排列，然后取top的那些negatives，使得negatives和positives的比例最多3:1。&lt;/p&gt;

&lt;h3 id=&#34;data-augmentation&#34;&gt;Data augmentation&lt;/h3&gt;

&lt;p&gt;本文针对object detection提出一种新的data augmentation的方法，sample a patch。sample完毕之后，对于每个gt box，如果它的中心仍然在这个patch中，则保留这个gt box。&lt;/p&gt;

&lt;h2 id=&#34;和yolo-v2的不同点&#34;&gt;和YOLO v2的不同点&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Gt boxes matching strategy不同；&lt;/li&gt;
&lt;li&gt;使用multi-scale feature maps的方式不同；&lt;/li&gt;
&lt;li&gt;使用了新型的data augmentation方法；&lt;/li&gt;
&lt;li&gt;default boxes或anchor数量不同；&lt;/li&gt;
&lt;li&gt;backbone network也不同；&lt;/li&gt;
&lt;li&gt;使用了hard negative mining;&lt;/li&gt;
&lt;li&gt;预测offsets的方式不同。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>YOLO v2</title>
      <link>https://x-lai.github.io/post/yolo-v2/</link>
      <pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://x-lai.github.io/post/yolo-v2/</guid>
      <description>

&lt;h1 id=&#34;yolo-v2-yolo9000&#34;&gt;YOLO v2: YOLO9000&lt;/h1&gt;

&lt;p&gt;和YOLO v1相比，YOLO v2做出了一些improvements，使得目标检测更快而且效果更好。&lt;/p&gt;

&lt;h2 id=&#34;better&#34;&gt;Better&lt;/h2&gt;

&lt;p&gt;1.Batch Normalization：获得more than 2% improvement；&lt;/p&gt;

&lt;p&gt;2.High Resolution Classifier：YOLO用的classifier network的输入原来是224*224的，只是后来为了检测而提升了448*448，这就意味着YOLO在训练的时候还需要adjust到新的输入分辨率上，造成训练的时候效果不好。而对于YOLO v2，在一开始的时候它就先在ImageNet上对classification network用448*448的分辨率进行fine tune 10个epochs 。这样就使得这个网络有时间来适应新的分辨率。然后再对得到的network用detection来正式地fine tune。这样可以获得接近4% mAP的提升。&lt;/p&gt;

&lt;p&gt;3.采用anchor boxes。像Faster R-CNN一样使用anchor，这样YOLO v2只需要预测offset，而不是直接预测一个box coordinates。本文提到比起预测coordinates，预测offsets更简化了问题，也使得网络更易于训练。同时，为了得到higher resolution output，减少了一层pooling layer。并且在预测anchor的时候，与YOLO不一样的是，YOLO v2会为同一个grid cell的不同的box predictor都预测对应的class，这样就使得输出的channels数量是$H*W*num\_anchors*(4+1+num\_classes)$。使用anchor虽然mAP有一点点下降，但是recall上升了，所以这样做提供了更大的优化空间。&lt;/p&gt;

&lt;p&gt;4.Dimension Clusters：在Faster R-CNN里，anchor的size和aspect ratio都是hand-picked，所以可能会误差较大。本文提出对训练集的所有的ground-truth box进行k-means聚类，来挑选k类不同size或aspect ratio的anchor。表示一个box和一群boxes的距离使用IOU如下表示：$d(box, centroid)=1-IOU(box, centroid)$，这样就使得被聚在一类的anchor boxes之间的IOU很大，甚至基本重合。所以，聚类产生的结果可以得到k种能够代表几乎所有boxes的size或aspect ratio了。通过比较训练集的bounding boxes与所用anchor之间average IOU，发现使用cluster产生的anchor时，k=5的效果和使用hand-picked anchors时k=9的效果是差不多的。由此可以发现dimension cluster的优势。&lt;/p&gt;

&lt;p&gt;5.Direct Location Prediction：使用了anchor之后，如果按照faster R-CNN的方法，在预测box的中心点(x,y)的时候，是用$x=t^x*w_a+x_a,y=t^y*h_a+y_a$计算得来的。这样的话，会造成在训练的开始阶段，预测的box在image的每一处都有可能出现，这种unconstrained的方法会导致模型的instability。本文就提出预测相对于grid cell的location的offsets。这样的话就可以将输出值限制在(0,1)之间。具体公式如下：$x=c_w\sigma{(t_x)}+c_x,y=c_h\sigma{(t_y)}+c_y,w=p_we^{t_w},h=p_he^{t_h}$，其中$c_x,c_y,c_w,c_h$分别表示grid cell的左上角的横纵坐标以及宽和高，$p_w,p_h$分别表示对应anchor的宽和高。$\sigma{()}$表示sigmoid函数。实验表明，这样做可以获得接近%5的提升。&lt;/p&gt;

&lt;p&gt;6.Fine-Grained Features：为了得到不同resolution的feature map，从而使得提取的特征更加完善，YOLO v2将前一个输出为26*26的layer作为passthrough layer，然后在具体实现时将它切成多个13*13的feature maps，这样再与之前的13*13的输出堆叠成多个feature maps一起进行预测。这样可以获得大约1%的提升。&lt;/p&gt;

&lt;p&gt;7.Multi-scale Training：这样可以使得network在不同resolution下预测detection，使网络更加generalized。&lt;/p&gt;

&lt;h2 id=&#34;faster&#34;&gt;Faster&lt;/h2&gt;

&lt;p&gt;提出Darknet-19作为backbone，使得计算量更小。&lt;/p&gt;

&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;

&lt;p&gt;如何标记ground-truth: 在YOLO v2的结构里，输出有$H*W*num\_anchors*(4+1+num\_classes)$个channels，因为和YOLO v1不一样，它对每个region都预测其class。那么输出一共有3类，分别是：&lt;/p&gt;

&lt;p&gt;1.localization：每个localization输出一个四元组$(\sigma{(t^x)}, \sigma{(t^y)}, t^w, t^h)$，其中$\sigma{(t^x)}=\frac{x-x_c}{w_c}, \sigma{(t^y)}=\frac{y-y_c}{h_c}$，$x,y$分别表示预测的bounding box的中心点的横纵坐标，$x_c, y_c$分别表示对应grid cell的left top的横纵坐标，$w_c, h_c$分别表示grid cell的宽和高; $t^w=ln{\frac{w}{w_a}}, t^h=ln{\frac{h}{h_a}}$，$w,h$分别表示预测的bounding box的宽和高，$w_a, h_a$分别表示对应的anchor的宽和高。所以通过以上公式即可在预测的四元组$(\sigma{(t^x)}, \sigma{(t^y)}, t^w, t^h)$和bounding box的四元组$(x,y,w,h)$之间相互转换。所以在localization方面，直接使用给定的ground-truth boxes来得到另一种表达形式；&lt;/p&gt;

&lt;p&gt;2.objectness：对于每一个localization，YOLO v2都会预测一个对应的objectness $\sigma{(t_o)}$。而且YOLO v2设定每个ground-truth box只由一个anchor来负责。首先，对于每个ground-truth box，先确定预测它由它的中心点落在的那个grid cell负责；其次，找到与该grid cell对应的所有anchors中与该ground-truth box的IOU最大的那一个anchor（待定，不确定是由anchor本身来算IOU，还是anchor对应的那个prediction box来算IOU），即由该anchor来负责预测该ground-truth bounding box。另外，对于那些与每个ground-truth box的IOU都小于threshold的prediction boxes，它们对应的ground-truth objectness设置为0。&lt;/p&gt;

&lt;p&gt;3.classification：对应于每个localization，YOLO v2还会预测它被分到哪一类。只需要考虑那些ground-truth boxes对应的localization即可，因为只有这些位置上的classification才会在计算loss的时候用上。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>YOLO v1</title>
      <link>https://x-lai.github.io/post/yolo-v1/</link>
      <pubDate>Sun, 08 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://x-lai.github.io/post/yolo-v1/</guid>
      <description>

&lt;h1 id=&#34;yolo-you-only-look-once&#34;&gt;YOLO: You Only Look Once&lt;/h1&gt;

&lt;p&gt;以往的目标检测器都是分成多个部分进行处理的，本文提出一种新的目标检测框架YOLO，通过单个神经网络直接根据full images来预测bounding boxes和class probabilities。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;YOLO框架有三个优点：&lt;/p&gt;

&lt;p&gt;1.这个架构非常快；&lt;/p&gt;

&lt;p&gt;2.YOLO虽然产生更多的localization errors，但是更少的false positive on background（即很少将background预测成object）。因为不像sliding windows或者region proposal-based的方法，YOLO是在整张图片上全局地进行预测，所以它会implicitly encodes context information，从而提高分类的准确度。&lt;/p&gt;

&lt;p&gt;3.YOLO可以学到非常general的representations of object。&lt;/p&gt;

&lt;h2 id=&#34;unified-detection&#34;&gt;Unified Detection&lt;/h2&gt;

&lt;p&gt;该系统首先将输入图片划分成$S \times S$的grid，如果一个object的中心落到了一个grid cell里面，那么该grid cell就负责检测该object。每个grid cell会预测B个bounding boxes以及对应的confidence scores，这些confidene scores反映了其分别对应的bounding box包含物体的可能性以及这个bounding box预测的准确性。所以本文将confidence定义为$ Pr(Object) * IOU^{truth}_{pred} $。其中$ IOU^{truth}_{pred} $表示当前预测出的bounding box与ground-truth bounding box的IOU。&lt;/p&gt;

&lt;h3 id=&#34;训练的时候如何得到ground-truth&#34;&gt;训练的时候如何得到ground truth：&lt;/h3&gt;

&lt;p&gt;每个bounding box包括5个预测值：x, y, w, h, confidence。先判断哪些grid cells中落入了object的中心，若某个grid cell有object，它的ground-truth confidence就是$IOU^{truth}_{pred}$，若某个grid cell没有object，那么它对应的ground-truth confidence就是0。此外，(x, y)是ground-truth的bounding box的中心相对于grid cell的边界的比例。w和h分别是ground-truth的bounding box相对于整个图片宽和高的比例。&lt;/p&gt;

&lt;h3 id=&#34;testing&#34;&gt;Testing：&lt;/h3&gt;

&lt;p&gt;此外，每个grid cell还预测C个conditional class probabilities, $Pr(Class_i|Object)$。不管B有多大，每个grid cell只会预测one set of class probabilities。&lt;/p&gt;

&lt;p&gt;在test的时候，将conditional class probabilities和confidence predictions相乘即可得到class-specific confidence scores。因为$ Pr(Class_i|Object)*Pr(Object)*IOU^{truth}_{pred}=Pr(Class_i)*IOU^{truth}_{pred} $。&lt;/p&gt;

&lt;p&gt;最终，对预测值进行nms，并过滤confidence score小于某阈值的prediction，得到最终目标检测结果。&lt;/p&gt;

&lt;h3 id=&#34;training&#34;&gt;Training:&lt;/h3&gt;

&lt;p&gt;YOLO在预测的时候，每个grid cells会预测B个bounding boxes，但是在训练时，只挑选其中之一来负责预测某一个object（对存在object的grid cells而言）。而挑选的依据就是挑选当前与ground-truth bounding box的IOU最大的那个（每次训练时不一定固定，是由当前输出动态决定的）。本文提出这样做会造成bounding box predictions之间的specialization（即相同的grid cell对应的不同predictors分别负责不同的sizes/aspect ratios/classes），提高整体的recall。&lt;/p&gt;

&lt;h4 id=&#34;loss-function&#34;&gt;Loss Function&lt;/h4&gt;

&lt;p&gt;使用pretrained ImageNet model进行fine tune。并且在设计loss function时，没有直接使用sum squared error。因为：&lt;/p&gt;

&lt;p&gt;1.sum squared error简单地将localization error和classification error的权重设为相同了；（改变coordinates的weight）&lt;/p&gt;

&lt;p&gt;2.在每个image中，有很多grid cells都不包含object，那么训练时会将这些grid cells的confidence scores都推向0，而这些grid cells的gradient常常会超过其他那些包含object的grid cells的gradient。所以这就导致了模型的不稳定性，使得训练在很早的时候就发散了。（降低non-object grid cells在loss function中所占weight）&lt;/p&gt;

&lt;p&gt;3.sum squared error也equally weights large boxes和small boxes。但是实际上相同的deviation对于large box和small box的影响是不同的。（使用开根号来减轻这种差异）&lt;/p&gt;

&lt;p&gt;由上，本文提出了一种新的loss function：&lt;/p&gt;

&lt;p&gt;$L=\lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B}1_{ij}^{obj}[(x_{ij}-\hat{x_{ij}})^2+(y_{ij}-\hat{y_{ij}})^2] \\ \quad +\lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} 1_{ij}^{obj} [(\sqrt{w_{ij}}-\sqrt{\hat{w_{ij}}})^2+(\sqrt{h_{ij}}-\sqrt{\hat{h_{ij}}})^2] \\ \quad +\sum_{i=0}^{S^2} \sum_{j=0}^{B} 1_{ij}^{obj} (C_{ij} - \hat{C_{ij}})^2 \\ \quad +\lambda_{noobj}\sum_{i=0}^{S^2} \sum_{j=0}^{B}1_{ij}^{noobj} (C_{ij}-\hat{C_{ij}})^2 \\ \quad + \sum_{i=0}^{S^2} 1_{i}^{obj} \sum_{c\in classes} (p_i ( c )-\hat{p_i ( c )})^2$&lt;/p&gt;

&lt;p&gt;在本文中，$\lambda_{coord}=5, \quad\lambda_{noobj}=0.5$。&lt;/p&gt;

&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;1.impose strong spatial constraints on bounding boxes. 因为每个grid cell只能预测2个boxes，和1个class。这样就限制了YOLO这个模型能够预测的相邻物体的数量。同时，也使得这个模型难以检测成群出现的小物体；&lt;/p&gt;

&lt;p&gt;2.难以generalize to训练时没见过的aspect ratios或configurations；&lt;/p&gt;

&lt;p&gt;3.该架构有许多downsampling layers，所以该模型使用了很多相对coarse的特征；&lt;/p&gt;

&lt;p&gt;4.尽管使用了开根号减轻相同error对不同大小的object的影响差异，但是这种差异仍然存在，并且对结果会有较大影响。&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.jianshu.com/p/cad68ca85e27&#34; target=&#34;_blank&#34;&gt;YOLO v1深入理解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/24916786&#34; target=&#34;_blank&#34;&gt;图解YOLO&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/guleileo/article/details/80581858&#34; target=&#34;_blank&#34;&gt;从YOLOv1到YOLOv3，目标检测的进化之路&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Faster R-CNN</title>
      <link>https://x-lai.github.io/post/faster-rcnn/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://x-lai.github.io/post/faster-rcnn/</guid>
      <description>

&lt;h1 id=&#34;faster-r-cnn&#34;&gt;Faster R-CNN&lt;/h1&gt;

&lt;p&gt;Fast R-CNN和SPP-net的速度相比于R-CNN而言，已经快了很多，但是仍然还有提升的空间，而且它们的时间瓶颈在于Region proposal上的耗时。所以Faster R-CNN提出使用RPN(Region Proposal Network)来得到region proposal，并且该RPN和detection network有一部分共享的convolutional layers，避免了feature map的重复计算，自然也就提高了速度。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Faster R-CNN分为两个modules：1）RPN，2）Fast R-CNN detector。&lt;/p&gt;

&lt;h2 id=&#34;region-proposal-networks&#34;&gt;Region Proposal Networks&lt;/h2&gt;

&lt;p&gt;RPN将一张任意size的图片作为输入，输出一个rectangular object proposal的集合，而且每个object proposal都附带一个objectness score（用以表示该proposal是object的程度）。&lt;/p&gt;

&lt;h3 id=&#34;anchor&#34;&gt;anchor&lt;/h3&gt;

&lt;p&gt;之前说到RPN和detection network有一部分共享的convolutional layers，所以当图片输入这部分layers之后，会输出一个feature map。为了得到region proposals，本文提出了anchor的设计。使用一个small network在这个feature map上slide。这个small network将n$\times$n（本文取n=3）的spatial window作为输入，每个spatial window将会被映射到一个低维的特征向量。这个向量将用于两个sibling fc layer：1）box-regression layer，2）box-classification layer。每个spatial window对应一个anchor，每个anchor有k种以该anchor location为中心的anchor-box（在本文中设定$k=3\ scale\times3\ aspect\ ratio=9$），box-classification layer输出一个2k维向量（即对应k个anchor-box是或不是object的score），用来判断对应anchor-box是否为object proposal。另外box-regression layer输出一个4k维向量（即对应k个anchor-box的四元组），用来对anchor-box的位置进行refine。&lt;/p&gt;

&lt;p&gt;这种anchor的设计，使得即便是single-scale的效果也是很好的，这样就大大减少了运行耗时。&lt;/p&gt;

&lt;h3 id=&#34;loss-function&#34;&gt;Loss function&lt;/h3&gt;

&lt;p&gt;在训练RPN时，需要确定每个anchor-box的label（即每个anchor-box是否包含object）。本文采用如下规则。将以下两种anchor标记为positive：1）与某一ground-truth box的IoU overlap最高的anchor，2）与某一ground-truth box的IoU overlap超过0.7。而与所有ground-truth box的IoU overlap小于0.3的anchor将被标记为negative。&lt;/p&gt;

&lt;p&gt;training的目标是最小化：$L(p, t)=\frac{1}{N_{cls}}\sum_i{L_{cls}(p_i,p_i^* )}+\lambda\frac{1}{N_{reg}}\sum_i{p_i^* L_{reg}(t_i,t_i^* )}$&lt;/p&gt;

&lt;p&gt;其中$p$表示classification layer输出的向量，每一维表示对应anchor是或不是object的概率。$p_i^* $表示label值，若对应anchor含object的话，则值为1，否则为0。$N_{cls}$表示mini-batch的大小（在本文中取256），$N_{reg}$表示anchor location的数量（在本文中约为2400），$\lambda$在本文中取10，$L_{cls}$表示是或不是object的log loss function，$L_{reg}=smooth_{L_1}(t_i-t_i^* )$。$t$表示regression layer输出的向量，$t=(t_x,t_y,t_w,t_h)，t^* =(t_x^* ,t_y^* ,t_w^* ,t_h^* )$，具体定义如下：&lt;/p&gt;

&lt;p&gt;$t_x=(x-x_a)/w_a,t_y=(y-y_a)/h_a,t_w=log(w/w_a),t_h=log(h/h_a)$&lt;/p&gt;

&lt;p&gt;$t_x^* =(x^* -x_a)/w_a,t_y^* =(y^* -y_a)/h_a,t_w^* =log(w^* /w_a),t_h^* =log(h^* /h_a)$&lt;/p&gt;

&lt;p&gt;其中$x,y,w,h$分别表示box的中心点坐标、宽度和高度，$x,x_a,x^*$分别表示预测的box、anchor的box和ground-truth的box（对$y,w,h$类似如此）。&lt;/p&gt;

&lt;h3 id=&#34;training-rpn-and-fast-r-cnn-detection-network&#34;&gt;Training RPN and Fast R-CNN detection network&lt;/h3&gt;

&lt;p&gt;由于RPN和detection network共享了一部分convolutional layers，所以在训练的时候，如果两部分network分开独立训练的话，不同的方式会有不同的效果。而且如果想要将这两部分network同时训练back propagate，有一个问题，即输入进detection network的bounding box coordinates实际上是输入图片的一个函数，而并非像之前使用selective search得到region proposal一样是固定的，所以如果要back propagate的话，要计算loss对bounding box coordinates的偏导，这是nontrivial的，将会浪费很多训练时间在back propagate上。所以为了加快训练的速度，将两部分network分开独立训练，从而得到一个可以接受的近似解才是现实的。&lt;/p&gt;

&lt;p&gt;本文提出一种alternating training。首先训练RPN，然后再使用当前已训练的RPN来获取proposal，并以此来训练Fast R-CNN的detection network，fine tune Fast R-CNN，然后使用Fast R-CNN的参数来初始化RPN，然后继续迭代这个过程。直到收敛。&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Fast R-CNN</title>
      <link>https://x-lai.github.io/post/fast-rcnn/</link>
      <pubDate>Wed, 10 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://x-lai.github.io/post/fast-rcnn/</guid>
      <description>

&lt;h1 id=&#34;fast-r-cnn&#34;&gt;Fast R-CNN&lt;/h1&gt;

&lt;p&gt;相比于classification，object detection任务准确度更低，难度更大，主要由于两个挑战：&lt;/p&gt;

&lt;p&gt;1.需要处理大量的proposal；&lt;/p&gt;

&lt;p&gt;2.在region proposal的rough localization的基础上需要refine以获得准确的localization。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;r-cnn-spp-net的缺点&#34;&gt;R-CNN/SPP-net的缺点：&lt;/h2&gt;

&lt;p&gt;1.R-CNN和SPP-net都是multi-stage的；&lt;/p&gt;

&lt;p&gt;2.R-CNN运行慢，而且训练时耗费时间和空间；&lt;/p&gt;

&lt;p&gt;3.SPP-net在fine-tune时不会更新spatial pyramid pooling之前的网络参数。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;和SPP-net类似，Fast R-CNN将整个图片作为输入，得到feature map，并得到每个region proposal在feature map中对应的window，经过RoI pooling layer得到长度固定的向量。之后，每个特征向量都会经过一系列fc层，最终分为两支：1）softmax层classifier，2）refined bounding-box position regressor。&lt;/p&gt;

&lt;h3 id=&#34;roi-pooling-layer&#34;&gt;RoI pooling layer&lt;/h3&gt;

&lt;p&gt;将h$\times$w的RoI window分割成H$\times$W grid of sub-windows（H和W都是hyperparameters），每个sub-window的大小约为h/H$\times$w/W，每个sub-window都通过max pooling得到一个response。每一个channel独立进行pooling。最终不同size的window都能输出成size为H$\times$W的向量。&lt;/p&gt;

&lt;h3 id=&#34;transformations-of-pre-trained-networks&#34;&gt;Transformations of pre-trained networks&lt;/h3&gt;

&lt;p&gt;从pre-trained network到新的模型需要经历3个transformations：&lt;/p&gt;

&lt;p&gt;1.最后一个max-pooling layer换成RoI Pooling layer;&lt;/p&gt;

&lt;p&gt;2.1000-way classifier要换成(K+1)-way classifier和bounding-box regressor;&lt;/p&gt;

&lt;p&gt;3.输入有两个：1）图片，2）RoI&lt;/p&gt;

&lt;h3 id=&#34;fine-tuning&#34;&gt;Fine-tuning&lt;/h3&gt;

&lt;p&gt;SPP-net在spp layer之前的层中的weights在fine-tuning的时候是不会更新的，这就有可能会限制识别的准确度。但是为什么不会更新呢？关键在于同一个batch中的sample（即SPP-net的RoI window）会来自于不同的图片，如果要更新在spp layer之前的weights，那么就需要首先得到每个sample对应的RoI window的receptive field。而一个window的receptive field会非常大，甚至常常是整个图片，所以当同一个batch的sample来自不同的图片时，在一个batch训练过程中就需要输入很多张图片，这会导致极度的inefficiency。&lt;/p&gt;

&lt;p&gt;本文提出了一种更加efficient的训练方法来改善这个问题：hierarchical sampling。即在训练的时候，为了得到一个batch，先sample N个images，然后再从其中每个image中sample R/N个RoI（R是batch size）。当N越小，一个batch的计算量就会越小（因为处理的图片少了，共享的计算多了）。在本文中取N=2，R=128。&lt;/p&gt;

&lt;p&gt;此外，和R-CNN、SPP-net不同的是，Fast R-CNN不需要分multi stages，只需要一个stage就能同时优化softmax classifier和bounding-box regressor。具体有：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Multi-task loss&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;$L(p,u,t^u,v)=L_{cls}(p,u)+\lambda[u \ge 1]L_{loc}(t^u,v)$&lt;/p&gt;

&lt;p&gt;Multi-task loss如上式所示。其中$L_{cls}(p,u)=-log{p_u}$，$L_{loc}(t^u,v)=\sum_{i \in {x,y,w,h}}{smooth_{L_1}(t_i^u-v_i)}$，$smooth_{L_1}(x)=\left\{\begin{aligned}
&amp;amp; 0.5x^2  &amp;amp; if \  |x| &amp;lt; 1 \\ &amp;amp; |x|-0.5  &amp;amp; otherwise
\end{aligned}
\right.$。&lt;/p&gt;

&lt;p&gt;$p$是一个K+1维向量，表示将该window分到K类中每一类的概率（此外还需再加上background类）。$u$和$v$分别表示该window被标注的ground-truth的类别和被标注的ground-truth的四元组$(x,y,w,h)$，$t$表示Fast R-CNN使用该window预测得到的各个类别对应的四元组，$t^u$表示类别$u$对应的四元组$(t_x^u, t_y^u,t_w^u,t_h^u)$。&lt;/p&gt;

&lt;h3 id=&#34;scale-invariance&#34;&gt;Scale invariance&lt;/h3&gt;

&lt;p&gt;有两种方式实现scale invariant object detection：1）brute force：每张图片都以一个预先设定的size作为输入进行处理，2）multi-scale approach：在test的时候，使用image pyramid来scale-normalize每个object proposal，在multi-scale training的时候，在每次采样图片的时候，随机选择一个pyramid scale进行训练（也作为一种data augmentation的方式）。&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>SPPNet</title>
      <link>https://x-lai.github.io/post/sppnet/</link>
      <pubDate>Wed, 10 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://x-lai.github.io/post/sppnet/</guid>
      <description>

&lt;h1 id=&#34;sppnet-spatial-pyramid-pooling-network&#34;&gt;SPPNet：spatial pyramid pooling network&lt;/h1&gt;

&lt;h2 id=&#34;spp&#34;&gt;SPP&lt;/h2&gt;

&lt;p&gt;spatial pyramid pooling：现存的cnn几乎都要求输入图片是固定size的。这种要求是人为的，而且可能会降低对图片或者任意size的子图片的识别准确度。SPP-net可以不管图片的size/scale都生成一个固定长度的向量。而且pyramid pooling还对物体变形非常健壮。&lt;/p&gt;

&lt;p&gt;对于一个conv层的输出（即多个filter对应的feature maps），对其采用spatial pyramid pooling，对每个feature map都使用多个bin（bin的scale随feature map的size而变化，从而使得不论feature map的size如何，number of bins都不变），每个bin覆盖的区域都将输出一个response。所以经过spp能够输出固定长度的向量。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;以往对于size不同的图片输入cnn，解决办法是使用warp或者crop，但是这可能会导致object的distortion或content loss，从而导致物体识别准确度下降。本文提出cnn对输入图片size固定的要求仅仅是由于fc层或classifier层导致的，conv层的卷积核filter是sliding window的，所以用于提取某种pattern，并不要求输入size是固定的。所以本文提出一种新的网络结构，在fc层之前加入spatial pyramid pooling，从而使网络不再要求输入图片的size固定。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;spatial-pyramid-pooling-layer的好处&#34;&gt;spatial pyramid pooling layer的好处&lt;/h2&gt;

&lt;p&gt;1.输入图片可以是任意scale的，所以在训练时，可以resize输入的图片，将输入图片转化成任意scale，再输入到相同的网络中。这样会使该网络能够提取在不同scale下的特征；&lt;/p&gt;

&lt;p&gt;2.the coarsest pyramid level（即对应spatial pyramid pooling layer中bin能够占据整个image的情况）实际上是一种global pooling操作，在其他工作中，被发现具有减少overfitting和提高准确度的作用，并且global max pooling被认为有助于weakly supervised object recognition.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;spp-net-for-object-detection&#34;&gt;SPP-net for object detection&lt;/h2&gt;

&lt;p&gt;在object detection任务上，相对于以前的方法，R-CNN准确度提升了很多。但是R-CNN是从原始图片中选取～2000个region proposals，所以对于每个region proposal都需要经过cnn提取feature，这非常耗时。sap-net提出先将原始图片经过一个cnn得到整体的feature map，再从feature map中提取~2000个region，并接着使用spatial pyramid pooling对不同size的region得到相同维度的向量。所以spp-net能够大大减少前向传播的时间。&lt;/p&gt;

&lt;h3 id=&#34;multi-scale-feature-extraction&#34;&gt;multi-scale feature extraction&lt;/h3&gt;

&lt;p&gt;使用multi-scale feature extraction可以进一步优化。本文提出resize图片到多个scale得到image pyramid，使$min(w,h)=s\in S=\{480,576,688,864,1200\}$（使用min函数可以保证feature window足够大）。再将此image pyramid作为cnn的输入得到feature maps。如何将这些不同scale的feature maps结合起来有两种方式。1）将这些不同的scale的feature maps堆叠在一起，然后channel-by-channel pooling到固定维度的向量，2）只选取其中一个scale使得scaled后的candidate window的number of pixels最接近224$\times$224，只需要使用这一个scale对应的feature map计算feature window并pooling到固定维度的向量用于训练即可。&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>R-CNN</title>
      <link>https://x-lai.github.io/post/r-cnn-regions-with-cnn-feature/</link>
      <pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://x-lai.github.io/post/r-cnn-regions-with-cnn-feature/</guid>
      <description>

&lt;h1 id=&#34;r-cnn-regions-with-cnn-feature&#34;&gt;R-CNN: Regions with CNN feature&lt;/h1&gt;

&lt;h3 id=&#34;1-人们可以将强大的卷积神经网络应用在自底向上的region-proposals上-用于定位和分割对象&#34;&gt;1.人们可以将强大的卷积神经网络应用在自底向上的region proposals上，用于定位和分割对象&lt;/h3&gt;

&lt;h3 id=&#34;2-当标记的训练数据稀少时-可以使用监督的一个辅助任务的预训练-加上某一domain-specific的fine-tuning-可以获得巨大的表现提升&#34;&gt;2.当标记的训练数据稀少时，可以使用监督的一个辅助任务的预训练，加上某一domain-specific的fine tuning，可以获得巨大的表现提升。&lt;/h3&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;使用rcnn来目标检测&#34;&gt;使用RCNN来目标检测&lt;/h3&gt;

&lt;h4 id=&#34;module-design&#34;&gt;module design&lt;/h4&gt;

&lt;p&gt;分为三个模块：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;region proposals: generate category-independent region proposals.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用Selective search生成候选region&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;feature vector: a cnn extracts a fixed-length feature vector from each proposed region&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从每个候选区域得到4096维向量。在计算过程中，需要首先将区域中的图像数据转化成与cnn兼容的形式（论文中使用的cnn要求输入size为227 $\times$ 227)。需要时将proposed region warp到需要的size。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;svm: a set of class-specific linear svms&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;test-time-detection&#34;&gt;test-time detection&lt;/h4&gt;

&lt;p&gt;在test阶段，每张图片都会生成～2000个region proposals，将每个proposal warp并输入到cnn中，得到固定大小的4096维特征向量。之后使用svm对其打分。给定一张图片的所有的打分的区域，对所有区域按照分值进行排序，然后对每一类都独立使用greedy non-maximum suppression（即如果某一区域和比该区域打分更高的某个区域（相同类别）之间的IoU overlap大于某个threshold，则拒绝该区域）。&lt;/p&gt;

&lt;h4 id=&#34;training&#34;&gt;training&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;supervised pre-training&lt;/em&gt; ：使用pre-training，使用auxiliary dataset(ILSVRC2012 classification / 1000类)训练一个分类模型。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;domain-specific fine-tuning&lt;/em&gt; ：将pre-training得到的模型中1000-way的classification layer换成随机初始化的(N+1)-way的classification layer。使用VOC dataset对新的模型fine-tuning。&lt;/p&gt;

&lt;p&gt;在训练过程中，首先从原始输入图片中生成～2000个region proposals，接着使用新的模型前向传播，从而fine-tune整个模型。但是，在前向传播之前，还需要确认每个region proposal的label。因为每个proposal的位置和数据集中已标注的对应object的ground-truth box的位置不太可能一模一样，所以在这里需要近似处理一下，得到每个region proposal的label。本文的做法是将和ground-truth box之间IoU大于等于0.5的region proposals的label视为positive，而将剩余的proposals标记为negative（即分为background类）。得到region proposals对应的label之后，需要以此为数据集训练cnn。本文提出一种size为128的mini-batch，在每个batch中，随机挑选32个positive样本和96个negative样本以构建size为128的batch。&lt;/p&gt;

&lt;p&gt;一旦特征被提取出来，并且训练的标签也得到了，我们就要对每一类确定linear svm。因为训练数据太大了以致于难以同时装入内存，本文采用了standard hard negative mining的方法。关于此方法，可参考&lt;a href=&#34;https://blog.csdn.net/qq_36570733/article/details/83444245&#34; target=&#34;_blank&#34;&gt;hard nagetive mining&lt;/a&gt;。大致是由于negative样本的数目远远大于positive样本数目，所以为了训练时避免向negative靠近（即防止测试时几乎全都分类到negative），只选取全体negative样本中的一个子集（在本文中取positive样本数:negative样本数为1:3）。但是，如果仅仅只是随机选取的话，可能会导致结果中一些本身是negative的样本被分类成了positive。所以hard negative mining的思路是从全体negative样本中选取hard negative样本作为子集进行训练。其中hard negative样本就是那些很容易被分类成positive的negative样本。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
