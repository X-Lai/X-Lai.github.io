[{"authors":["admin"],"categories":null,"content":"Currently, I am a fourth-year undergraduate student, majoring in Computer Science and Technology in Harbin Institute of Technology(HIT). Before that, I was an exchange student in Computer Science Department of the University of Hong Kong(HKU).\nI am interested in Computer Vision and Deep Learning. And I will pursue my Ph.D in the Chinese University of Hong Kong (CUHK) starting in 2020.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://x-lai.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Currently, I am a fourth-year undergraduate student, majoring in Computer Science and Technology in Harbin Institute of Technology(HIT). Before that, I was an exchange student in Computer Science Department of the University of Hong Kong(HKU).\nI am interested in Computer Vision and Deep Learning. And I will pursue my Ph.D in the Chinese University of Hong Kong (CUHK) starting in 2020.","tags":null,"title":"Xin LAI","type":"authors"},{"authors":null,"categories":null,"content":" Mask RCNN Mask RCNN基于Faster RCNN，在Head处又加了一个mask branch，使得该框架能够适用于instance segmentation。这是一种instance-first的实例分割方法（另一种是segmentation-first strategy），先得到bbox，再对每个bbox进行分割。\nRoIAlign Faster RCNN的RoIPool层会有一定的quantization，造成misalignment误差，所以Mask RCNN采用了RoIAlign通过bilinear interpolation来重新采样并计算出样本subpixel的具体值再聚集（maximum或average）得到最终的输出值，这样达到了alignment的目的，也减少了误差，使得该框架适用于position sensitive的任务（localization/mask）。\nDecouple mask and class prediction 以往实例分割算法的做法是per-pixel的multi-class categorization，但Mask RCNN将mask任务和classificaion任务分割开，mask head的输出是K个$m\\times m$的binary mask map（K为目标种类数），但最终是用sigmoid而非softmax（实验表明用sigmoid效果好很多），并且mask结果只用其中第$k$个mask map($k$表示class head的预测结果)。这也表明，一旦某个instance已经作为一个整体被分类好了之后，在mask阶段就不用再考虑class的问题了，这也将会使得训练更加容易。\n最终的mask loss是通过将预测的$m\\times m$的mask map resize到RoI的size，然后再做pixel-to-pixel的loss。\n","date":1572998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572998400,"objectID":"c1ae8fd02b63fd857d212cb8e4e99f24","permalink":"https://x-lai.github.io/post/mask-rcnn/","publishdate":"2019-11-06T00:00:00Z","relpermalink":"/post/mask-rcnn/","section":"post","summary":"Mask RCNN Mask RCNN基于Faster RCNN，在Head处又加了一个mask branch，使得该框架能够适用于instance segmentati","tags":["object detection","instance segmentation"],"title":"Mask RCNN","type":"post"},{"authors":null,"categories":null,"content":" Single-Shot Refinement Neural Network for Object Detection RefineDet综合了two-stage和one-stage的优点，同时规避其缺点，是一种既准又快的目标检测器。two-stage的检测器有3个优点：1）two-stage structure with sampling heuristics帮助解决class imbalance问题; 2) 有两次对bbox的回归；3）有两阶段对特征的表示（把两个阶段看作两个任务，可以把整个学习看作多任务学习，所以第一个阶段的学习会加强浅层特征）。RefineDet就借助了这几点，能够在第一阶段reject掉easy negative examples，并且先进行一个粗略的bbox regress和pos/neg的打分，再从第二阶段进行精细的regression和classification。\nArchitecture 类似于FPN，bottom-up部分称为ARM（anchor refinement module），top-down部分称作ODM（object detection module）。两个module都将各个level的feature maps用于预测和分类。ARM进行bbox的粗略预测和binary classification，从而得到coarse bbox坐标和pos/neg打分。然后挑出negative confidence大于某个threshold的那些easy negative examples，再把剩下的anchor送到第二阶段ODM处理。\n和Faster RCNN对比，以及一些思考 1.RefineDet的第一阶段ARM是在FPN的bottom-up部分做的，而Faster RCNN的第一阶段是在top-down部分做的，所以将ARM放在top-down做是否会更好？\n2.RefineDet将easy negative examples放到后面ODM处理时是不计算它们的损失，从而缓解class imbalance的影响。而Faster RCNN的第二阶段是用roi把每个proposal都用RoI pooling割成固定长度的向量，所以效率低。而前者仍然基于one-stage的框架来做。\n3.把整个regression分两步做，是不是会让第一阶段ARM的训练使得浅层的特征加强，从而导致整体效果的增长？如果是这样的话，是不是其他任务也可以分步来做（用来加强特征），或者再多加一步变成分3步？或者在更浅的层（backbone的部分）再加一步？\n","date":1572998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572998400,"objectID":"cf22830e6e0a72b4b34a1f3b88e94d32","permalink":"https://x-lai.github.io/post/refinedet/","publishdate":"2019-11-06T00:00:00Z","relpermalink":"/post/refinedet/","section":"post","summary":"Single-Shot Refinement Neural Network for Object Detection RefineDet综合了two-stage和one-stage的优点，同时规避其缺点，是一种既准又快的目标检测器。two-st","tags":["object detection"],"title":"RefineDet","type":"post"},{"authors":null,"categories":null,"content":" DenseBox: Unifying Landmark Localization with End to End Object Detection DenseBox直接通过FCN框架来同时预测定位以及分类。它主要有2个方面值得学习：1）DenseBox不需要手动设置anchor，根据receptive field来设置ground-truth，通过一种类似于segmentation的方法来训练；2）通过multi-task learning加入landmarks信息，提高检测精度。\n框架 输入$m\\times n$的image, 输出5个$\\frac{m}{4} \\times \\frac{n}{4}$的feature maps。其中一个用来输出object confidence，其他四个用来标记bounding box。对于output feature map的每个pixel$(x_i, y_i)$，通过一个五元组来描述bounding box$(\\hat{s},\\hat{dx^t}=\\hat{x_i}-x_t,\\hat{dy^t}=\\hat{y_i}-y_t,\\hat{dx^b}=\\hat{x_i}-x_b, \\hat{dy^b}=\\hat{y_i}-y_b)$。其中$\\hat{s}$是object confidence，$(x_t,y_t)$是bounding box的左上角坐标，$(x_b,y_b)$是bounding box的右下角坐标。\n如何标记ground-truth：在output feature map上，对于一个人脸的bounding box，取其长度的0.3作为半径，并以其中心为圆心，作一个圆，该圆的区域为positive，其他区域为negative。\n为什么这样设计ground-truth？因为在该圆的区域内的每个pixel的receptive field都大致是以这个object（即人脸）为中心的。\nMulti-task training 没使用landmarks的情况下，其loss function为：\n$L_{det}(\\theta)=\\sum_i({M(\\hat{t_i})L_{cls}(\\hat{t_i},y_i^*)}+\\lambda_{loc}[y_i^*\u0026gt;0]L_{loc}(\\hat{d_i}, d_i^*))$\n使用landmarks之后，再添加一个分支（该分支也用上了用于detection的特征图），该分支输出N个feature maps（其中N表示每个object有N个landmarks）。loss function变为：\n$L_{full}(\\theta)=\\lambda_{det}L_{det}(\\theta)+\\lambda_{lm}L_{lm}(\\theta)+L_{rf}(\\theta)$\n其中$L_{lm}(\\theta)$表示landmark regression loss（用于回归的，与$L_{loc}$类似），$L_{rf}(\\theta)$表示refined detection loss（用于分类的，与$L_{cls}$类似）。\n","date":1572220800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572220800,"objectID":"5296b565d5d2e837fda3cb95db4135a9","permalink":"https://x-lai.github.io/post/densebox/","publishdate":"2019-10-28T00:00:00Z","relpermalink":"/post/densebox/","section":"post","summary":"DenseBox: Unifying Landmark Localization with End to End Object Detection DenseBox直接通过FCN框架来同时预测定位以及分类。它主要有2个方面值得学习：1）DenseBox不需要手动设置a","tags":["object detection"],"title":"DenseBox","type":"post"},{"authors":null,"categories":null,"content":" Object detection via a multi-region \u0026amp; semantic segmentation-aware CNN model 本文提出一种新的CNN模型，包括了multi-region CNN model以及semantic segmentation-aware CNN model。同时还提出一种iterative localization通过不断迭代得到精确的localization regression。\nMulti-Region CNN Model 整体框架包括两部分:\n1) Activation maps module: 用于从整幅输入图像中提取整体的特征图；\n2) Region adaptation module: 用于从整体特征图中截取proposal region各个部分的特征，并使用spatially adaptive (max-) pooling，然后分别放入对应的网络中。\n最终，candidate box representation通过将所有region adaptation module的最后一层concatenate起来得到。\n这样做，是为了：1）强制得到各个每个RoI各个部分的表示，从而得到一个much richer and more robust object representation；2）使得最终的object representation more sensitive to inaccurate localization。\nRole in detection 1) Discriminative feature diversification. 使用多个区域而不是整个RoI的好处在于可以使得网络集中于某一块小区域，而非一整块区域。经过实验证明，遮住一部分而将注意力集中在一小块能使得检测准确率上升；\n2) Localization-aware representation. 使用多区域的结构可以缓解定位不准的问题，因为使用多区域会使得检测器专注于某几小块，从而对定位更加敏感（即如果定位不准确的话，会对分类产生较大偏差）。此外，使用多区域实际上impose soft constraints regarding the visual content allowed in each type of region，这种soft constraints对定位有一定帮助。\nSemantic Segmentation-Aware CNN Model 由于segmentation任务和detection任务相似，本文提出使用semantic segmentation任务来提取特征，用来辅助检测，而且，这种方法不需要额外的分割所需的ground truth annotated data。\n使用FCN训练分割模型。直接将object detection的每个object对应的bounding box内的像素设置为foreground class。其余设为background class。以此获得训练数据。训练完成后，将最后一层分类层去掉，并用剩下的来提取semantic segmentation-aware features.\nObject Localization CNN region adaptation module for bounding box regression 在上述模型中再额外添加一个module用来预测bounding box。对于每一类，都输出4个数。并且为了能够refine那些与真实相差较大的box，本文提出先将原region扩大1.3倍，然后再预测。\nIterative Localization \u0026amp; Bounding Box Voting 详见原文\n一些想法 能否通过mask来学到不同的部位，并结合其特征，并实现end-to-end的训练？\n","date":1572134400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572134400,"objectID":"10b126678a3ed2b2361e8bf629fb96f4","permalink":"https://x-lai.github.io/post/mrcnn/","publishdate":"2019-10-27T00:00:00Z","relpermalink":"/post/mrcnn/","section":"post","summary":"Object detection via a multi-region \u0026amp; semantic segmentation-aware CNN model 本文提出一种新的CNN模型，包括了multi-region CNN model以及semantic segmentation-aware CNN model。同时还提出一种","tags":["object detection"],"title":"MRCNN","type":"post"},{"authors":null,"categories":null,"content":" Fully Convolutional Network for Semantic Segmentation 本文提出FCN，这是第一个对pixelwise prediction端到端训练FCN的工作。training和inference都是通过dense feedforward computation和backpropagation从而达到whole-image-at-a-time的。\nAdapting classifiers for dense prediction 将classification network的fc层解释为卷积层，只不过卷积核的大小和输入feature map的大小一致。这样重新解释使得原fc层的输出也具有一种spatial信息，只不过每个channel都是$1\\times 1$的。另外，classification network的subsampling会使得输出变得更加coarse。\n几种upsampling的方法 Shift-and-stitch is filter dilation Shift-and-stitch是一种用于解决下采样导致的resolution降低的问题。假设下采样的factor是f，对于每对(x,y)（$0\\le x,y \\le f$），先将输入向右移动x个像素，并向下移动y个像素，这样就得到了$f^2$个新的输入了。分别对这$f^2$个输入进行处理（经过下采样，之后可能还经过卷积等处理），最终分别得到$f^2$个输出。然后再将这$f^2$个输出交错排列，形成最终输出的feature map，使得该feature map上每个像素都对应着它们的receptive field的中心。具体可参考博客关于FCN 论文中的 Shift-and-stitch 的详尽解释。\n这样做shift-and-stitch看上去处理了$f^2$次，降低了处理效率。但实际上有一些技巧可以避免计算$f^2$次。例如a trous。\n其实，shift-and-stitch与卷积核放大（filter dilation）的效果是一样的。只是需要将将卷积核放大一倍的同时做以下处理：\n$f\u0026rsquo;_{ij}=\\left\\{\\begin{aligned} \u0026amp; f_{i/s,j/s} \u0026amp; 如果s能整除i和j \\\\ \u0026amp; 0 \u0026amp; otherwise \\end{aligned} \\right.$\n这样做相当于是一种带孔的卷积。深入了解可参考博客shift-and-stitch理解。\nUpsamling is (fractionally strided) convolution 另外一种将coarse output转向dense pixels的方法是插值（interpolation）。例如，simple bilinear interpolation从输入map的每相邻4个输入来计算对应的输出$y_{ij}$。\n$y_{ij}=\\sum_{\\alpha,\\beta=0}^{1}|1-\\alpha-\\{i/f\\}||1-\\beta-\\{j/f\\}|x_{\\lfloor i/f\\rfloor+\\alpha,\\lfloor j/f \\rfloor +\\beta}$\n其中{}表示小数部分，f是upsampling factor。\n根据这个公式，实际上这是一种stride是分数的卷积。例如，当f=2时，这就相当于stride=0.5时的卷积，比如$y_{00}=x_{00}, y_{11}=0.25(x_{00}+x_{01}+x_{10}+x_{11}),y_{12}=0.5(x_{01}+x_{11})$。\n在本文中，这种方法称作in-network upsampling，实验证明这种方法既快又好。\nSegmentation Architecture 使用ILSVRC预训练模型，并使用in-network upsampling以及pixelwise loss来进行dense prediction。另外，还在不同层之间添加了skips来将coarse, semantic和local, appearance information结合起来，用以提升semantics和spatical precision。使用相对较浅的层做more local prediction很自然，因为它们的receptive fields较小。\nlayer fusion实质上是elementwise operation。不同层之间点与点的对应需要resampling和padding。本文将较小scale的层通过upsampling达到与较大scale层相同，从而进行elementwise operation。\nelementwise operation首先通过contatenation来fuse features，然后再使用$1\\times 1$的convolution。\nSkip Architectures for Segmentation 将pool3和放大两倍的pool4，以及放大四倍的conv7，做一个fusion。再将结果放大8倍的到最终的输出。其中upsampling layer的filter使用bilinear interpolation来初始化，根据in-network sampling（即fractionally strided convolution），这个filter的参数不是固定的，可以学到。\nMetrics pixel accuracy: $\\sum_i n_{ii}/\\sum_i t_i$\nmean accuracy: $(1/n_{cl})\\sum_i n_{ii}/t_i$\nmean IU: $(1/n_{cl})\\sum_i n_{ii}/(t_i + \\sum_j n_{ji} - n_{ii})$\nfrequency weighted IU: $(\\sum_k t_k)^{-1}\\sum_i t_i n_{ii}/(t_i+\\sum_j n_{ji}-n_{ii})$\n其中$n_{ij}$是将真实第i类的像素预测到第j类的像素数，$n_{cl}$是种类数，$t_i=\\sum_j{n_{ij}}$是真实为第i类的像素数。\n","date":1571875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571875200,"objectID":"ad9708d60bff591f7dfdbae1936298a7","permalink":"https://x-lai.github.io/post/fcn/","publishdate":"2019-10-24T00:00:00Z","relpermalink":"/post/fcn/","section":"post","summary":"Fully Convolutional Network for Semantic Segmentation 本文提出FCN，这是第一个对pixelwise prediction端到端训练FCN的工作。training和inference都","tags":["semantic segmentation"],"title":"FCN","type":"post"},{"authors":null,"categories":null,"content":" R-FCN: Object Detection via Region-based Fully Convolutional Networks Faster R-CNN之所以比较慢，是因为RPN输出的每个proposal都需要经过detection network。本文提出position-sensitive score maps使得RPN输出的所有的proposal共享计算，从而提高效率。\nFast R-CNN或Faster R-CNN能够具有很强的translation variance的能力，主要在于RoI pooling对信息的整合，并形成一个fixed-size的而且携带位置信息(因为RoI pooling layer将feature map spatially分成若干块)的向量。但这样是以unshared per-RoI computation为代价的。（原文说得更加透彻：This region-specific operation(指RoI pooling) breaks down translation invariance, and the post-RoI convolutional layers are no longer translation-invariant when evaluated across different regions.）\nArchitecture R-FCN的所有learnable weight layers都是convolutional的，并且是在entire image中计算的。最后一个卷积层对每一类都产生$k^2$个position-sensitive score maps，所以输出层有$k^2(C+1)$个channel（+1是为了预测出background）。$k^2$是对应了每个RoI都被切割成$k^2$块分别给对应的位置。\nR-FCN以position-sensitive RoI pooling layer结束。对每个RoI，position-sensitive RoI pooling layer将最后一个卷积层的输出聚集起来，并且打分。每个RoI经过这个pooling layer之后都会得到$C+1$个channel，并且每个channel都是$k\\times k$的。每个channel都对应着一个类。而且对每个channel，这$k\\times k$个bin中的每一个都只来自于R-FCN输出的$k^2$个position-sensitive score maps中的一个。\nPosition-sensitive score maps \u0026amp; Position-sensitive RoI pooling 将每个RoI矩形划分成$k \\times k$个bins。对于一个形如$w\\times h$的矩形，每个bin的大小约等于$\\frac{w}{k}\\times \\frac{h}{k}$。在第$(i,j)$个bin$(0\\le i,j \\le k-1)$中，定义position-sensitive RoI pooling如下：\n$r_c(i,j|\\Theta)=\\sum_{(x,y)\\in bin(i,j)}{z_{i,j,c}(x+x_0,y+y_0|\\Theta)}/n$\n其中$r_c(i,j)$表示pooling的结果中对第c类的第$(i,j)$个bin的值，$z_{i,j,c}$表示那$k^2(C+1)$个score maps中第c类第$(i,j)$个bin对应的那个，$(x_0,y_0)$表示RoI的左上角在score map上的坐标。本文采用的是average pooling，所以除以n，当然也可以用max pooling。\n最后，对pooling得到的C+1个channel分别做average voting。即$r_c(\\Theta)=\\sum_{i,j}{r_c(i,j|\\Theta)}$。\n同样的，除了上述$k^2(C+1)$维的卷积层，还有一个$4k^2$的卷积层和它平行，用于预测坐标。positive-sensitive RoI pooling作用在这$4k^2$个channel上，得到4个大小为$k^2$的channel，最终通过average voting得到4维向量即$(t_x, t_y, t_w, t_h)$。\n","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"ad2c3e9f33b4a7faeffea5b9cfc2dad4","permalink":"https://x-lai.github.io/post/r-fcn/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/post/r-fcn/","section":"post","summary":"R-FCN: Object Detection via Region-based Fully Convolutional Networks Faster R-CNN之所以比较慢，是因为RPN输出的每个proposal都需要经过detection network。本文提出posi","tags":["object detection"],"title":"R-FCN","type":"post"},{"authors":null,"categories":null,"content":" U-Net: Convolutional Networks for Biomedical Image Segmentation 现在训练一个深度网络需要很多已标注的训练样本。本文提出一种网络结构以及训练技巧，并大大依靠data augmentation，使用有限的标注样本高效训练。该网络结构包括一个用来捕捉context的contracting path，以及一个对称的用来准确定位的expanding path。实验表明用很少的数据就能end-to-end训练出一个效果很好的网络。\nArchitecture 本文以FCN为基础，对contracting path进行补充，添加了一个expanding path，并且为了精确定位，和来自contracting path的高分辨feature map结合起来形成upsampled output。（为什么要先contract在expand？借助了auto-encoder的思想，中间将特征维度降低，是为了将有用的特征聚集起来，并且排除一些噪声或者low-level的信息。）\nOverlap-tile Strategy U-Net使用的卷积操作的pad=0，所以每次卷积完之后，会使得feature map的size减少2。所以最终经过contracting path和expanding path之后输出的feature map会比输入图像的size少很多。所以需要将输入的最原始的图像的size扩大，使得最终输出的feature map和最原始的图像size相同。那么如何扩大输入的图像呢？本文提出通过将最原始图像的边缘做一个镜像操作来扩大它的size。\nData augmentation 除了overlap-tile strategy，本文还提出对有限的图像使用elastic deformation，进而达到data augmentation的目的。通过$3\\times 3$的grid使用random displacement得到smooth deformation。因为本文的背景是细胞的分割，所以使用elastic deformation是合理的，而且还能使得网络学到一种对这种deformation的invariance。此外，shift和rotation以及gray value invariance都可以通过类似的augmentation方法学到。\nTraining 实质上，分割任务就是对每个像素都进行分类，所以最终的输出为$K$个feature maps。同时，为了处理touching objects（同类objects叠在一起）的边界处，最终损失函数还需要对这种边界处的像素提高权重。所以最终损失函数（也叫energy function）为：$E=\\sum_{x\\in \\Omega}{w(x)log(p_{l(x)}(x))}$ 。其中$l(x)$表示像素x的真实label。$p_k(x)$表示像素x在第$k$类的概率值。\n其中weight map $w(x)$通过以下公式计算:$$w(x)=w_c(x)+w_0\\cdot exp(-\\frac{(d_1(x)+d_2(x))^2}{2\\sigma^2})$$\n其中$w_c$用于平衡class frequencies。$d_1(x)$表示像素x到最近细胞的边界的距离，$d_2(x)$表示像素x到第二近的细胞的边界的距离。在实验中，设置$w_0=10,\\sigma=5$。\n","date":1571270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571270400,"objectID":"9991cb8e58dd802a2d7e0721b148357a","permalink":"https://x-lai.github.io/post/u-net/","publishdate":"2019-10-17T00:00:00Z","relpermalink":"/post/u-net/","section":"post","summary":"U-Net: Convolutional Networks for Biomedical Image Segmentation 现在训练一个深度网络需要很多已标注的训练样本。本文提出一种网络结构以及训练技巧，并大大依靠data augmentation，使","tags":["semantic segmentation"],"title":"U-Net","type":"post"},{"authors":null,"categories":null,"content":" Feature Pyramid Networks for Object Detection feature pyramids对于识别不同scale的物体有着重要的作用。但是当feature map随着网络的深度由于subsampling(pooling)的操作而变得越来越小时，虽然构建了feature pyramids，但是深度较浅的feature map尽管resolution更大，但low-level信息比较多而且semantics较少（low-level信息多是不利的，semantics少也是不利的），而深度较深的feature map尽管low-level信息较少semantics较多，但resolution很小（这不适于预测小物体），所以造成这种feature pyramids效果的瓶颈。本文提出通过一个带有lateral connections的top-down网络结构，用于构建既有fine resolution又富含semantics的feature maps（即每个level的feature map都是富含semantics的）。\nFeature Pyramid Networks 整个网络结构包含两部分:1) Bottom-up pathway, 2) Top-down pathway and lateral connections\nBottom-up pathway 这是feature pyramids的自底至上的subsampling的过程。每个scale不止一层，而是多个卷积层。并且将一个scale对应的多个层（称为stage）的最后一层的输出作为该scale的代表。每个scale的scaling step是2，也即相邻scale的feature map的size之比为2。\nTop-down pathway and lateral connections Top-down pathway是将coarse-resolution的feature map逐级地upsampling得到higher resolution的过程。这种upsampling在本文中直接使用nearest neighbor upsampling。在upsampling（upsampling factor=2）的同时，还会结合（element-wise addition操作）一个来自于bottom-up pathway的pyramids中相同size的feature map的lateral connection（需要将这个来自bottom-up的feature map进行$1 \\times 1$的卷积以便改变channel dimensions）。upsampling的结果虽然resolution更加fine了，semantics也很多，但是由于upsampling的随机性，又是less accurately localized。而来自于bottom-up的feature map却恰恰相反，虽然更加accurately localized，但是semantics更少。所以将他们结合起来，会得到既accurately localized又semantically strong的feature map。\nFeature Pyramid Networks for RPN 对于使用FPN的RPN，本文设计将所有anchor分散到不同scale的feature map上，以便feature pyramids上的每个scale都有自己的分工。并且对不同的scale的feature map，RPN的head参数都是共享的。实验表明，当不共享head参数时，准确度是相似的。这也说明FPN产生的不同level的feature maps有similar semantic levels。\n","date":1570838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570838400,"objectID":"594571fca5ca47c00ff3c7dc7b2d273e","permalink":"https://x-lai.github.io/post/fpn/","publishdate":"2019-10-12T00:00:00Z","relpermalink":"/post/fpn/","section":"post","summary":"Feature Pyramid Networks for Object Detection feature pyramids对于识别不同scale的物体有着重要的作用。但是当feature map随着网络的深度由于subsampling","tags":["object detection"],"title":"FPN","type":"post"},{"authors":null,"categories":null,"content":" Focal Loss for Dense Object Detection one-stage的检测器比two-stage的更快更简单，但是在准确度上落后。本文发现最主要的原因在于极度的foreground与background的比例失衡（class imbalance）。由此，本文对标准的cross entropy进行修改，并提出Focal Loss。这种loss function可以减小easy examples占的整体loss的比重。这样就使得模型在hard examples下训练，从而减小大量easy examples对训练的影响，解决class imbalance的问题。此外，本文还提出RetinaNet。\nR-CNN这种two-stage的检测器如何解决class imbalance？  a two-stage cascade。像R-CNN这种two-stage的模型先region proposal，然后再对proposal的结果进行分类。这中间形成了一种级联，使得在第一步region proposal的过程（如RPN，selective search）中就筛除了大量的easy examples（这种过程也可视为一种sampling heuristics）。\n 在第二阶段分类的时候，也使用了sampling heuristics（如fixed foreground-background ratio 1:3 和online hard example mining）。\n  而one-stage detector却需要处理更多的candidate object locations，通常会有～100k个candidate locations。尽管one-stage的检测器也可以使用上述第2点的sampling heuristics，但是由于基数过大，还是会造成训练过程被easy examples主导，由此导致低效以及模型的泛化能力降低。\nFocal Loss 对于二分类而言，cross entropy表达式如下：\n$$CE(p,y)=\\left\\{ \\begin{aligned} \u0026amp; -log(p) \u0026amp; \u0026amp; if \\ y=1 \\\\ \u0026amp; -log(1-p) \u0026amp; \u0026amp; otherwise \\end{aligned}\\right.$$\n其中p表示预测到y=1的那一类的概率。为了方便，定义$p_t$:\n$$p_t=\\left\\{\\begin{aligned} \u0026amp; p \u0026amp; \u0026amp; if \\ y=1 \\\\ \u0026amp; 1-p \u0026amp; \u0026amp; otherwise \\end{aligned}\\right.$$\nBalanced Cross Entropy Balanced Cross Entropy对标准cross entropy进行了改造：\n$CE(p_t)=-\\alpha_tlog(p_t)$\n其中当y=1时，$\\alpha_t=\\alpha$，而当y=-1时，$\\alpha_t=1-\\alpha$。\n这样做之后，可以通过调节$\\alpha$的值来调节positive-negtive samples分别对loss的贡献的比例。但是这样并不能区分hard examples和easy examples对loss的贡献。所以引入了Focal Loss。\nFocal Loss $FL(p_t)=-(1-p_t)^\\gamma log(p_t)$\nFocal Loss在cross entropy的基础上添加了一个权重$(1-p_t)^\\gamma$。这个权重可用于调整不同样本对loss的贡献。对于y=-1而言，一个easy example的预测结果将会使$p_t$增大，从而使得其权重$(1-p_t)^\\gamma$减小，所以easy example的权重会减小。\n在实践中，Focal loss通常会与balanced cross entropy结合起来:\n$FL(p_t)=-\\alpha_t(1-p_t)^\\gamma log(p_t)$\nClass Imbalance and Model Initialization 二分类通常会默认将模型初始化使得起初分类到y=1和y=-1的概率相等。但是在class imbalance的情况下，将两类的初始预测值设为相同会使得frequent class的loss主导整个loss，使得训练不稳定。于是，本文设计将rare class初始预测概率设定为一个prior $\\pi$，在本文中$\\pi=0.01$。这样设定之后，在class imbalance的情况下，正负两类所占的loss相差就不会太大。这样的initialization会使得训练过程更加smooth。\nRetinaNet RetinaNet采用FPN的结构，使用ResNet作为backbone。并且延续了RPN的anchor设计。值得一提的是，对于每个level的feature maps，所有的classification subnet和box subnet都是共享参数的。并且class subnet和box subnet都是使用了$3\\times3$的卷积核。但是class subnet和box subnet并不共享参数。另外，class subnet的最后一层是sigmoid，而不是softmax。这样处理的目的是为了兼容一个anchor对应多个标签。然后损失函数相当于对每个类别都进行二分类。\n","date":1570665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570665600,"objectID":"b9d2e30741ad993c63477bab6871185b","permalink":"https://x-lai.github.io/post/retinanet/","publishdate":"2019-10-10T00:00:00Z","relpermalink":"/post/retinanet/","section":"post","summary":"Focal Loss for Dense Object Detection one-stage的检测器比two-stage的更快更简单，但是在准确度上落后。本文发现最主要的原因在于极度的foregroun","tags":["object detection"],"title":"RetinaNet","type":"post"},{"authors":null,"categories":null,"content":" SSD: Single Shot MultiBox Detector 和YOLO一样，SSD也是一种one-stage的object detector。SSD将prediction bounding boxes的输出空间离散化到多个feature maps上每个位置的多个不同scale和aspect ratios的default boxes上。\nFramework SSD在训练时只需要输入图片和对每个object的gt boxes。SSD将会在不同resolution的feature maps上来预测每个location使用哪个scale和aspect ratio的default boxes。对于每个default box，SSD将会预测shape offsets以及object category confidences（包括background类）。在训练的时候，先将这些default boxes match到gt boxes上，确定哪些default boxes是positive（即被认为是包含object的），剩余的default boxes就被作为negatives。然后再结合prediction来计算loss，进行训练。\n如何match default boxes 与YOLO v2不同的是，SSD提出不一定要对每个gt box只match一个default box(anchor box)。SSD先将每个gt box match到与它自己IOU最大的那个default box上，然后再将那些与任一gt box的IOU超过某个threshold（本文设为0.5）的default boxes也标记对应到该gt box/objectness/class。\nMulti-scale feature maps for detection 在预测的时候，SSD使用多种不同scale的feature maps来进行detection。\nConvolutional predictor for detection 对于所有不同scale的feature map，都将采用若干个3*3的filter来进行卷积，从而进行预测。\nDefault boxes and aspect ratios 对于feature map的每个cell的每个default box，SSD都会预测offsets以及classification。这样每种feature map就会有$(c+4)k$个filters（假设一共有c-1个class，另外加上background共有c个class）。其中k为default boxes的个数（本文取6）。\nTraining Training Objective 根据之前‘如何match default boxes’所述，我们可以获得哪些default boxes是positives或negatives。随后即可计算loss function。$L(x,c,l,g)=\\frac{1}{N} (L_{conf}(x,c)+\\alpha{L_{loc}(x,l,g)})$。其中$x_{ij}^k$要么取1，要么取0，表示第i个default box是否和第j个gt box(其category为k) match，$c,l,g$分别表示预测的class，预测的box以及预测的gt box。$L_{loc}$是使用和faster R-CNN类似的localization error。而$L_{conf}(x,c)=-\\sum_{i\\in Positives}^{N}x_{ij}^plog(\\hat{c_i^p})-\\sum_{i\\in Negatives}{log(\\hat{c_i^0})}, \\hat{c_i^p}=\\frac{exp(c_i^p)}{\\sum_p{exp(c_i^p)}}$。\nChoose scales and aspect ratios for default boxes 设共有m个不同scale的feature maps，那么每个feature map的default boxes的scale就用如下公式计算：$s_k=s_{min}+\\frac{s_{max}-s_{min}}{m-1}(k-1),\\quad k\\in[1,m]$。本文取$s_{min}=0.2, s_{max}=0.9$。另外再取aspect ratios。$a_r\\in \\{1,2,3,\\frac{1}{2},\\frac{1}{3}\\}$，第k个feature map的default box的宽$w_k^a$和高$h_k^a$分别用$w_k^a=s_k\\sqrt{a_r},h_k^a=s_k\\sqrt{a_r}$来计算。另外对于aspect ratio=1的情况，还要再加一种size：$s_{k}\u0026lsquo;=\\sqrt{s_ks_{k+1}}$。\nHard negative mining 对所有negative default boxes对应的confidence按高到低排列，然后取top的那些negatives，使得negatives和positives的比例最多3:1。\nData augmentation 本文针对object detection提出一种新的data augmentation的方法，sample a patch。sample完毕之后，对于每个gt box，如果它的中心仍然在这个patch中，则保留这个gt box。\n和YOLO v2的不同点  Gt boxes matching strategy不同； 使用multi-scale feature maps的方式不同； 使用了新型的data augmentation方法； default boxes或anchor数量不同； backbone network也不同； 使用了hard negative mining; 预测offsets的方式不同。  ","date":1568246400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568246400,"objectID":"cc1afbe91749c13bae09bfc749e87cc3","permalink":"https://x-lai.github.io/post/ssd/","publishdate":"2019-09-12T00:00:00Z","relpermalink":"/post/ssd/","section":"post","summary":"SSD: Single Shot MultiBox Detector 和YOLO一样，SSD也是一种one-stage的object detector。SSD将prediction bounding boxes的输出空间","tags":["object detection"],"title":"SSD","type":"post"},{"authors":null,"categories":null,"content":" YOLO v2: YOLO9000 和YOLO v1相比，YOLO v2做出了一些improvements，使得目标检测更快而且效果更好。\nBetter 1.Batch Normalization：获得more than 2% improvement；\n2.High Resolution Classifier：YOLO用的classifier network的输入原来是224*224的，只是后来为了检测而提升了448*448，这就意味着YOLO在训练的时候还需要adjust到新的输入分辨率上，造成训练的时候效果不好。而对于YOLO v2，在一开始的时候它就先在ImageNet上对classification network用448*448的分辨率进行fine tune 10个epochs 。这样就使得这个网络有时间来适应新的分辨率。然后再对得到的network用detection来正式地fine tune。这样可以获得接近4% mAP的提升。\n3.采用anchor boxes。像Faster R-CNN一样使用anchor，这样YOLO v2只需要预测offset，而不是直接预测一个box coordinates。本文提到比起预测coordinates，预测offsets更简化了问题，也使得网络更易于训练。同时，为了得到higher resolution output，减少了一层pooling layer。并且在预测anchor的时候，与YOLO不一样的是，YOLO v2会为同一个grid cell的不同的box predictor都预测对应的class，这样就使得输出的channels数量是$H*W*num\\_anchors*(4+1+num\\_classes)$。使用anchor虽然mAP有一点点下降，但是recall上升了，所以这样做提供了更大的优化空间。\n4.Dimension Clusters：在Faster R-CNN里，anchor的size和aspect ratio都是hand-picked，所以可能会误差较大。本文提出对训练集的所有的ground-truth box进行k-means聚类，来挑选k类不同size或aspect ratio的anchor。表示一个box和一群boxes的距离使用IOU如下表示：$d(box, centroid)=1-IOU(box, centroid)$，这样就使得被聚在一类的anchor boxes之间的IOU很大，甚至基本重合。所以，聚类产生的结果可以得到k种能够代表几乎所有boxes的size或aspect ratio了。通过比较训练集的bounding boxes与所用anchor之间average IOU，发现使用cluster产生的anchor时，k=5的效果和使用hand-picked anchors时k=9的效果是差不多的。由此可以发现dimension cluster的优势。\n5.Direct Location Prediction：使用了anchor之后，如果按照faster R-CNN的方法，在预测box的中心点(x,y)的时候，是用$x=t^x*w_a+x_a,y=t^y*h_a+y_a$计算得来的。这样的话，会造成在训练的开始阶段，预测的box在image的每一处都有可能出现，这种unconstrained的方法会导致模型的instability。本文就提出预测相对于grid cell的location的offsets。这样的话就可以将输出值限制在(0,1)之间。具体公式如下：$x=c_w\\sigma{(t_x)}+c_x,y=c_h\\sigma{(t_y)}+c_y,w=p_we^{t_w},h=p_he^{t_h}$，其中$c_x,c_y,c_w,c_h$分别表示grid cell的左上角的横纵坐标以及宽和高，$p_w,p_h$分别表示对应anchor的宽和高。$\\sigma{()}$表示sigmoid函数。实验表明，这样做可以获得接近%5的提升。\n6.Fine-Grained Features：为了得到不同resolution的feature map，从而使得提取的特征更加完善，YOLO v2将前一个输出为26*26的layer作为passthrough layer，然后在具体实现时将它切成多个13*13的feature maps，这样再与之前的13*13的输出堆叠成多个feature maps一起进行预测。这样可以获得大约1%的提升。\n7.Multi-scale Training：这样可以使得network在不同resolution下预测detection，使网络更加generalized。\nFaster 提出Darknet-19作为backbone，使得计算量更小。\nTraining 如何标记ground-truth: 在YOLO v2的结构里，输出有$H*W*num\\_anchors*(4+1+num\\_classes)$个channels，因为和YOLO v1不一样，它对每个region都预测其class。那么输出一共有3类，分别是：\n1.localization：每个localization输出一个四元组$(\\sigma{(t^x)}, \\sigma{(t^y)}, t^w, t^h)$，其中$\\sigma{(t^x)}=\\frac{x-x_c}{w_c}, \\sigma{(t^y)}=\\frac{y-y_c}{h_c}$，$x,y$分别表示预测的bounding box的中心点的横纵坐标，$x_c, y_c$分别表示对应grid cell的left top的横纵坐标，$w_c, h_c$分别表示grid cell的宽和高; $t^w=ln{\\frac{w}{w_a}}, t^h=ln{\\frac{h}{h_a}}$，$w,h$分别表示预测的bounding box的宽和高，$w_a, h_a$分别表示对应的anchor的宽和高。所以通过以上公式即可在预测的四元组$(\\sigma{(t^x)}, \\sigma{(t^y)}, t^w, t^h)$和bounding box的四元组$(x,y,w,h)$之间相互转换。所以在localization方面，直接使用给定的ground-truth boxes来得到另一种表达形式；\n2.objectness：对于每一个localization，YOLO v2都会预测一个对应的objectness $\\sigma{(t_o)}$。而且YOLO v2设定每个ground-truth box只由一个anchor来负责。首先，对于每个ground-truth box，先确定预测它由它的中心点落在的那个grid cell负责；其次，找到与该grid cell对应的所有anchors中与该ground-truth box的IOU最大的那一个anchor（待定，不确定是由anchor本身来算IOU，还是anchor对应的那个prediction box来算IOU），即由该anchor来负责预测该ground-truth bounding box。另外，对于那些与每个ground-truth box的IOU都小于threshold的prediction boxes，它们对应的ground-truth objectness设置为0。\n3.classification：对应于每个localization，YOLO v2还会预测它被分到哪一类。只需要考虑那些ground-truth boxes对应的localization即可，因为只有这些位置上的classification才会在计算loss的时候用上。\n","date":1568160000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568160000,"objectID":"a86e44e0d58440111b3c24cbfc34569f","permalink":"https://x-lai.github.io/post/yolo-v2/","publishdate":"2019-09-11T00:00:00Z","relpermalink":"/post/yolo-v2/","section":"post","summary":"YOLO v2: YOLO9000 和YOLO v1相比，YOLO v2做出了一些improvements，使得目标检测更快而且效果更好。 Better 1.Batch Normalization：获得","tags":["object detection"],"title":"YOLO v2","type":"post"},{"authors":null,"categories":null,"content":" YOLO: You Only Look Once 以往的目标检测器都是分成多个部分进行处理的，本文提出一种新的目标检测框架YOLO，通过单个神经网络直接根据full images来预测bounding boxes和class probabilities。\nYOLO框架有三个优点：\n1.这个架构非常快；\n2.YOLO虽然产生更多的localization errors，但是更少的false positive on background（即很少将background预测成object）。因为不像sliding windows或者region proposal-based的方法，YOLO是在整张图片上全局地进行预测，所以它会implicitly encodes context information，从而提高分类的准确度。\n3.YOLO可以学到非常general的representations of object。\nUnified Detection 该系统首先将输入图片划分成$S \\times S$的grid，如果一个object的中心落到了一个grid cell里面，那么该grid cell就负责检测该object。每个grid cell会预测B个bounding boxes以及对应的confidence scores，这些confidene scores反映了其分别对应的bounding box包含物体的可能性以及这个bounding box预测的准确性。所以本文将confidence定义为$ Pr(Object) * IOU^{truth}_{pred} $。其中$ IOU^{truth}_{pred} $表示当前预测出的bounding box与ground-truth bounding box的IOU。\n训练的时候如何得到ground truth： 每个bounding box包括5个预测值：x, y, w, h, confidence。先判断哪些grid cells中落入了object的中心，若某个grid cell有object，它的ground-truth confidence就是$IOU^{truth}_{pred}$，若某个grid cell没有object，那么它对应的ground-truth confidence就是0。此外，(x, y)是ground-truth的bounding box的中心相对于grid cell的边界的比例。w和h分别是ground-truth的bounding box相对于整个图片宽和高的比例。\nTesting： 此外，每个grid cell还预测C个conditional class probabilities, $Pr(Class_i|Object)$。不管B有多大，每个grid cell只会预测one set of class probabilities。\n在test的时候，将conditional class probabilities和confidence predictions相乘即可得到class-specific confidence scores。因为$ Pr(Class_i|Object)*Pr(Object)*IOU^{truth}_{pred}=Pr(Class_i)*IOU^{truth}_{pred} $。\n最终，对预测值进行nms，并过滤confidence score小于某阈值的prediction，得到最终目标检测结果。\nTraining: YOLO在预测的时候，每个grid cells会预测B个bounding boxes，但是在训练时，只挑选其中之一来负责预测某一个object（对存在object的grid cells而言）。而挑选的依据就是挑选当前与ground-truth bounding box的IOU最大的那个（每次训练时不一定固定，是由当前输出动态决定的）。本文提出这样做会造成bounding box predictions之间的specialization（即相同的grid cell对应的不同predictors分别负责不同的sizes/aspect ratios/classes），提高整体的recall。\nLoss Function 使用pretrained ImageNet model进行fine tune。并且在设计loss function时，没有直接使用sum squared error。因为：\n1.sum squared error简单地将localization error和classification error的权重设为相同了；（改变coordinates的weight）\n2.在每个image中，有很多grid cells都不包含object，那么训练时会将这些grid cells的confidence scores都推向0，而这些grid cells的gradient常常会超过其他那些包含object的grid cells的gradient。所以这就导致了模型的不稳定性，使得训练在很早的时候就发散了。（降低non-object grid cells在loss function中所占weight）\n3.sum squared error也equally weights large boxes和small boxes。但是实际上相同的deviation对于large box和small box的影响是不同的。（使用开根号来减轻这种差异）\n由上，本文提出了一种新的loss function：\n$L=\\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B}1_{ij}^{obj}[(x_{ij}-\\hat{x_{ij}})^2+(y_{ij}-\\hat{y_{ij}})^2] \\\\ \\quad +\\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} 1_{ij}^{obj} [(\\sqrt{w_{ij}}-\\sqrt{\\hat{w_{ij}}})^2+(\\sqrt{h_{ij}}-\\sqrt{\\hat{h_{ij}}})^2] \\\\ \\quad +\\sum_{i=0}^{S^2} \\sum_{j=0}^{B} 1_{ij}^{obj} (C_{ij} - \\hat{C_{ij}})^2 \\\\ \\quad +\\lambda_{noobj}\\sum_{i=0}^{S^2} \\sum_{j=0}^{B}1_{ij}^{noobj} (C_{ij}-\\hat{C_{ij}})^2 \\\\ \\quad + \\sum_{i=0}^{S^2} 1_{i}^{obj} \\sum_{c\\in classes} (p_i ( c )-\\hat{p_i ( c )})^2$\n在本文中，$\\lambda_{coord}=5, \\quad\\lambda_{noobj}=0.5$。\nLimitations 1.impose strong spatial constraints on bounding boxes. 因为每个grid cell只能预测2个boxes，和1个class。这样就限制了YOLO这个模型能够预测的相邻物体的数量。同时，也使得这个模型难以检测成群出现的小物体；\n2.难以generalize to训练时没见过的aspect ratios或configurations；\n3.该架构有许多downsampling layers，所以该模型使用了很多相对coarse的特征；\n4.尽管使用了开根号减轻相同error对不同大小的object的影响差异，但是这种差异仍然存在，并且对结果会有较大影响。\n参考 YOLO v1深入理解\n图解YOLO\n从YOLOv1到YOLOv3，目标检测的进化之路\n","date":1567900800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567900800,"objectID":"88db9a7547560e9287b55fbc2f8acd99","permalink":"https://x-lai.github.io/post/yolo-v1/","publishdate":"2019-09-08T00:00:00Z","relpermalink":"/post/yolo-v1/","section":"post","summary":"YOLO: You Only Look Once 以往的目标检测器都是分成多个部分进行处理的，本文提出一种新的目标检测框架YOLO，通过单个神经网络直接根据full images来预","tags":["object detection"],"title":"YOLO v1","type":"post"},{"authors":null,"categories":null,"content":" Faster R-CNN Fast R-CNN和SPP-net的速度相比于R-CNN而言，已经快了很多，但是仍然还有提升的空间，而且它们的时间瓶颈在于Region proposal上的耗时。所以Faster R-CNN提出使用RPN(Region Proposal Network)来得到region proposal，并且该RPN和detection network有一部分共享的convolutional layers，避免了feature map的重复计算，自然也就提高了速度。\nFaster R-CNN分为两个modules：1）RPN，2）Fast R-CNN detector。\nRegion Proposal Networks RPN将一张任意size的图片作为输入，输出一个rectangular object proposal的集合，而且每个object proposal都附带一个objectness score（用以表示该proposal是object的程度）。\nanchor 之前说到RPN和detection network有一部分共享的convolutional layers，所以当图片输入这部分layers之后，会输出一个feature map。为了得到region proposals，本文提出了anchor的设计。使用一个small network在这个feature map上slide。这个small network将n$\\times$n（本文取n=3）的spatial window作为输入，每个spatial window将会被映射到一个低维的特征向量。这个向量将用于两个sibling fc layer：1）box-regression layer，2）box-classification layer。每个spatial window对应一个anchor，每个anchor有k种以该anchor location为中心的anchor-box（在本文中设定$k=3\\ scale\\times3\\ aspect\\ ratio=9$），box-classification layer输出一个2k维向量（即对应k个anchor-box是或不是object的score），用来判断对应anchor-box是否为object proposal。另外box-regression layer输出一个4k维向量（即对应k个anchor-box的四元组），用来对anchor-box的位置进行refine。\n这种anchor的设计，使得即便是single-scale的效果也是很好的，这样就大大减少了运行耗时。\nLoss function 在训练RPN时，需要确定每个anchor-box的label（即每个anchor-box是否包含object）。本文采用如下规则。将以下两种anchor标记为positive：1）与某一ground-truth box的IoU overlap最高的anchor，2）与某一ground-truth box的IoU overlap超过0.7。而与所有ground-truth box的IoU overlap小于0.3的anchor将被标记为negative。\ntraining的目标是最小化：$L(p, t)=\\frac{1}{N_{cls}}\\sum_i{L_{cls}(p_i,p_i^* )}+\\lambda\\frac{1}{N_{reg}}\\sum_i{p_i^* L_{reg}(t_i,t_i^* )}$\n其中$p$表示classification layer输出的向量，每一维表示对应anchor是或不是object的概率。$p_i^* $表示label值，若对应anchor含object的话，则值为1，否则为0。$N_{cls}$表示mini-batch的大小（在本文中取256），$N_{reg}$表示anchor location的数量（在本文中约为2400），$\\lambda$在本文中取10，$L_{cls}$表示是或不是object的log loss function，$L_{reg}=smooth_{L_1}(t_i-t_i^* )$。$t$表示regression layer输出的向量，$t=(t_x,t_y,t_w,t_h)，t^* =(t_x^* ,t_y^* ,t_w^* ,t_h^* )$，具体定义如下：\n$t_x=(x-x_a)/w_a,t_y=(y-y_a)/h_a,t_w=log(w/w_a),t_h=log(h/h_a)$\n$t_x^* =(x^* -x_a)/w_a,t_y^* =(y^* -y_a)/h_a,t_w^* =log(w^* /w_a),t_h^* =log(h^* /h_a)$\n其中$x,y,w,h$分别表示box的中心点坐标、宽度和高度，$x,x_a,x^*$分别表示预测的box、anchor的box和ground-truth的box（对$y,w,h$类似如此）。\nTraining RPN and Fast R-CNN detection network 由于RPN和detection network共享了一部分convolutional layers，所以在训练的时候，如果两部分network分开独立训练的话，不同的方式会有不同的效果。而且如果想要将这两部分network同时训练back propagate，有一个问题，即输入进detection network的bounding box coordinates实际上是输入图片的一个函数，而并非像之前使用selective search得到region proposal一样是固定的，所以如果要back propagate的话，要计算loss对bounding box coordinates的偏导，这是nontrivial的，将会浪费很多训练时间在back propagate上。所以为了加快训练的速度，将两部分network分开独立训练，从而得到一个可以接受的近似解才是现实的。\n本文提出一种alternating training。首先训练RPN，然后再使用当前已训练的RPN来获取proposal，并以此来训练Fast R-CNN的detection network，fine tune Fast R-CNN，然后使用Fast R-CNN的参数来初始化RPN，然后继续迭代这个过程。直到收敛。\n","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"570780159d70d49bc3ad74b8aa7055ac","permalink":"https://x-lai.github.io/post/faster-rcnn/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/faster-rcnn/","section":"post","summary":"Faster R-CNN Fast R-CNN和SPP-net的速度相比于R-CNN而言，已经快了很多，但是仍然还有提升的空间，而且它们的时间瓶颈在于Region prop","tags":["object detection"],"title":"Faster R-CNN","type":"post"},{"authors":null,"categories":null,"content":" Fast R-CNN 相比于classification，object detection任务准确度更低，难度更大，主要由于两个挑战：\n1.需要处理大量的proposal；\n2.在region proposal的rough localization的基础上需要refine以获得准确的localization。\nR-CNN/SPP-net的缺点： 1.R-CNN和SPP-net都是multi-stage的；\n2.R-CNN运行慢，而且训练时耗费时间和空间；\n3.SPP-net在fine-tune时不会更新spatial pyramid pooling之前的网络参数。\nArchitecture 和SPP-net类似，Fast R-CNN将整个图片作为输入，得到feature map，并得到每个region proposal在feature map中对应的window，经过RoI pooling layer得到长度固定的向量。之后，每个特征向量都会经过一系列fc层，最终分为两支：1）softmax层classifier，2）refined bounding-box position regressor。\nRoI pooling layer 将h$\\times$w的RoI window分割成H$\\times$W grid of sub-windows（H和W都是hyperparameters），每个sub-window的大小约为h/H$\\times$w/W，每个sub-window都通过max pooling得到一个response。每一个channel独立进行pooling。最终不同size的window都能输出成size为H$\\times$W的向量。\nTransformations of pre-trained networks 从pre-trained network到新的模型需要经历3个transformations：\n1.最后一个max-pooling layer换成RoI Pooling layer;\n2.1000-way classifier要换成(K+1)-way classifier和bounding-box regressor;\n3.输入有两个：1）图片，2）RoI\nFine-tuning SPP-net在spp layer之前的层中的weights在fine-tuning的时候是不会更新的，这就有可能会限制识别的准确度。但是为什么不会更新呢？关键在于同一个batch中的sample（即SPP-net的RoI window）会来自于不同的图片，如果要更新在spp layer之前的weights，那么就需要首先得到每个sample对应的RoI window的receptive field。而一个window的receptive field会非常大，甚至常常是整个图片，所以当同一个batch的sample来自不同的图片时，在一个batch训练过程中就需要输入很多张图片，这会导致极度的inefficiency。\n本文提出了一种更加efficient的训练方法来改善这个问题：hierarchical sampling。即在训练的时候，为了得到一个batch，先sample N个images，然后再从其中每个image中sample R/N个RoI（R是batch size）。当N越小，一个batch的计算量就会越小（因为处理的图片少了，共享的计算多了）。在本文中取N=2，R=128。\n此外，和R-CNN、SPP-net不同的是，Fast R-CNN不需要分multi stages，只需要一个stage就能同时优化softmax classifier和bounding-box regressor。具体有：\nMulti-task loss\n$L(p,u,t^u,v)=L_{cls}(p,u)+\\lambda[u \\ge 1]L_{loc}(t^u,v)$\nMulti-task loss如上式所示。其中$L_{cls}(p,u)=-log{p_u}$，$L_{loc}(t^u,v)=\\sum_{i \\in {x,y,w,h}}{smooth_{L_1}(t_i^u-v_i)}$，$smooth_{L_1}(x)=\\left\\{\\begin{aligned} \u0026amp; 0.5x^2 \u0026amp; if \\ |x| \u0026lt; 1 \\\\ \u0026amp; |x|-0.5 \u0026amp; otherwise \\end{aligned} \\right.$。\n$p$是一个K+1维向量，表示将该window分到K类中每一类的概率（此外还需再加上background类）。$u$和$v$分别表示该window被标注的ground-truth的类别和被标注的ground-truth的四元组$(x,y,w,h)$，$t$表示Fast R-CNN使用该window预测得到的各个类别对应的四元组，$t^u$表示类别$u$对应的四元组$(t_x^u, t_y^u,t_w^u,t_h^u)$。\nScale invariance 有两种方式实现scale invariant object detection：1）brute force：每张图片都以一个预先设定的size作为输入进行处理，2）multi-scale approach：在test的时候，使用image pyramid来scale-normalize每个object proposal，在multi-scale training的时候，在每次采样图片的时候，随机选择一个pyramid scale进行训练（也作为一种data augmentation的方式）。\n","date":1562716800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562716800,"objectID":"7221b0e5baffdb36a50d73faf1ed3d3a","permalink":"https://x-lai.github.io/post/fast-rcnn/","publishdate":"2019-07-10T00:00:00Z","relpermalink":"/post/fast-rcnn/","section":"post","summary":"Fast R-CNN 相比于classification，object detection任务准确度更低，难度更大，主要由于两个挑战： 1.需要处理大量的prop","tags":["object detection"],"title":"Fast R-CNN","type":"post"},{"authors":null,"categories":null,"content":" SPPNet：spatial pyramid pooling network SPP spatial pyramid pooling：现存的cnn几乎都要求输入图片是固定size的。这种要求是人为的，而且可能会降低对图片或者任意size的子图片的识别准确度。SPP-net可以不管图片的size/scale都生成一个固定长度的向量。而且pyramid pooling还对物体变形非常健壮。\n对于一个conv层的输出（即多个filter对应的feature maps），对其采用spatial pyramid pooling，对每个feature map都使用多个bin（bin的scale随feature map的size而变化，从而使得不论feature map的size如何，number of bins都不变），每个bin覆盖的区域都将输出一个response。所以经过spp能够输出固定长度的向量。\n以往对于size不同的图片输入cnn，解决办法是使用warp或者crop，但是这可能会导致object的distortion或content loss，从而导致物体识别准确度下降。本文提出cnn对输入图片size固定的要求仅仅是由于fc层或classifier层导致的，conv层的卷积核filter是sliding window的，所以用于提取某种pattern，并不要求输入size是固定的。所以本文提出一种新的网络结构，在fc层之前加入spatial pyramid pooling，从而使网络不再要求输入图片的size固定。\nspatial pyramid pooling layer的好处 1.输入图片可以是任意scale的，所以在训练时，可以resize输入的图片，将输入图片转化成任意scale，再输入到相同的网络中。这样会使该网络能够提取在不同scale下的特征；\n2.the coarsest pyramid level（即对应spatial pyramid pooling layer中bin能够占据整个image的情况）实际上是一种global pooling操作，在其他工作中，被发现具有减少overfitting和提高准确度的作用，并且global max pooling被认为有助于weakly supervised object recognition.\nSPP-net for object detection 在object detection任务上，相对于以前的方法，R-CNN准确度提升了很多。但是R-CNN是从原始图片中选取～2000个region proposals，所以对于每个region proposal都需要经过cnn提取feature，这非常耗时。sap-net提出先将原始图片经过一个cnn得到整体的feature map，再从feature map中提取~2000个region，并接着使用spatial pyramid pooling对不同size的region得到相同维度的向量。所以spp-net能够大大减少前向传播的时间。\nmulti-scale feature extraction 使用multi-scale feature extraction可以进一步优化。本文提出resize图片到多个scale得到image pyramid，使$min(w,h)=s\\in S=\\{480,576,688,864,1200\\}$（使用min函数可以保证feature window足够大）。再将此image pyramid作为cnn的输入得到feature maps。如何将这些不同scale的feature maps结合起来有两种方式。1）将这些不同的scale的feature maps堆叠在一起，然后channel-by-channel pooling到固定维度的向量，2）只选取其中一个scale使得scaled后的candidate window的number of pixels最接近224$\\times$224，只需要使用这一个scale对应的feature map计算feature window并pooling到固定维度的向量用于训练即可。\n","date":1562716800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562716800,"objectID":"0d340e342862312fe3df3c7ed0267a77","permalink":"https://x-lai.github.io/post/sppnet/","publishdate":"2019-07-10T00:00:00Z","relpermalink":"/post/sppnet/","section":"post","summary":"SPPNet：spatial pyramid pooling network SPP spatial pyramid pooling：现存的cnn几乎都要求输入图片是固定size的。这种要求是人为的，而且可能会降低对图","tags":["object detection"],"title":"SPPNet","type":"post"},{"authors":null,"categories":null,"content":" R-CNN: Regions with CNN feature 1.人们可以将强大的卷积神经网络应用在自底向上的region proposals上，用于定位和分割对象 2.当标记的训练数据稀少时，可以使用监督的一个辅助任务的预训练，加上某一domain-specific的fine tuning，可以获得巨大的表现提升。 使用RCNN来目标检测 module design 分为三个模块：\n region proposals: generate category-independent region proposals.  使用Selective search生成候选region\n feature vector: a cnn extracts a fixed-length feature vector from each proposed region  从每个候选区域得到4096维向量。在计算过程中，需要首先将区域中的图像数据转化成与cnn兼容的形式（论文中使用的cnn要求输入size为227 $\\times$ 227)。需要时将proposed region warp到需要的size。\n svm: a set of class-specific linear svms  test-time detection 在test阶段，每张图片都会生成～2000个region proposals，将每个proposal warp并输入到cnn中，得到固定大小的4096维特征向量。之后使用svm对其打分。给定一张图片的所有的打分的区域，对所有区域按照分值进行排序，然后对每一类都独立使用greedy non-maximum suppression（即如果某一区域和比该区域打分更高的某个区域（相同类别）之间的IoU overlap大于某个threshold，则拒绝该区域）。\ntraining supervised pre-training ：使用pre-training，使用auxiliary dataset(ILSVRC2012 classification / 1000类)训练一个分类模型。\ndomain-specific fine-tuning ：将pre-training得到的模型中1000-way的classification layer换成随机初始化的(N+1)-way的classification layer。使用VOC dataset对新的模型fine-tuning。\n在训练过程中，首先从原始输入图片中生成～2000个region proposals，接着使用新的模型前向传播，从而fine-tune整个模型。但是，在前向传播之前，还需要确认每个region proposal的label。因为每个proposal的位置和数据集中已标注的对应object的ground-truth box的位置不太可能一模一样，所以在这里需要近似处理一下，得到每个region proposal的label。本文的做法是将和ground-truth box之间IoU大于等于0.5的region proposals的label视为positive，而将剩余的proposals标记为negative（即分为background类）。得到region proposals对应的label之后，需要以此为数据集训练cnn。本文提出一种size为128的mini-batch，在每个batch中，随机挑选32个positive样本和96个negative样本以构建size为128的batch。\n一旦特征被提取出来，并且训练的标签也得到了，我们就要对每一类确定linear svm。因为训练数据太大了以致于难以同时装入内存，本文采用了standard hard negative mining的方法。关于此方法，可参考hard nagetive mining。大致是由于negative样本的数目远远大于positive样本数目，所以为了训练时避免向negative靠近（即防止测试时几乎全都分类到negative），只选取全体negative样本中的一个子集（在本文中取positive样本数:negative样本数为1:3）。但是，如果仅仅只是随机选取的话，可能会导致结果中一些本身是negative的样本被分类成了positive。所以hard negative mining的思路是从全体negative样本中选取hard negative样本作为子集进行训练。其中hard negative样本就是那些很容易被分类成positive的negative样本。\n","date":1562544000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562544000,"objectID":"84421d18ea2d3b7c46123145d799b12b","permalink":"https://x-lai.github.io/post/r-cnn-regions-with-cnn-feature/","publishdate":"2019-07-08T00:00:00Z","relpermalink":"/post/r-cnn-regions-with-cnn-feature/","section":"post","summary":"R-CNN: Regions with CNN feature 1.人们可以将强大的卷积神经网络应用在自底向上的region proposals上，用于定位和分割对象 2.当标记的训练数据稀少时，可","tags":["object detection"],"title":"R-CNN","type":"post"}]